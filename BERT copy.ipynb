{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/bert-for-measuring-text-similarity-eec91c6bf9e1\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/similarity-metrics-in-nlp-acc0777e234c \n",
    "\n",
    "https://www.youtube.com/watch?v=Ey81KfQ3PQU\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model semantic search: 'multi-qa-MiniLM-L6-cos-v1'\n",
    "\n",
    "Model assymetric : 'msmarco-MiniLM-L-6-v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nova' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nova ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from Functions.connection.connection import *\n",
    "from Functions.AlertP1.data_cleaning import *\n",
    "from Functions.AlertP1.features import *\n",
    "from Functions.analysis.step_analysis import *\n",
    "from Functions.AlertP1.dummy_features import *\n",
    "from Functions.Models.Logistic_regression import *\n",
    "from Functions.Models.evaluation import *\n",
    "from Data_with_NLP import *\n",
    "import pandas as pd\n",
    "# import pandasql as ps\n",
    "import matplotlib.pyplot as plt\n",
    "# Import argsort\n",
    "from numpy import argsort\n",
    "from Functions.connection.connection import *\n",
    "from Functions.AlertP1.data_cleaning import *\n",
    "from Functions.AlertP1.features import *\n",
    "from Functions.analysis.step_analysis import *\n",
    "from Functions.AlertP1.dummy_features import *\n",
    "from Functions.Models.Logistic_regression import *\n",
    "from Functions.Models.evaluation import *\n",
    "from Data_with_NLP import *\n",
    "from Functions.pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nova' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nova ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "protocols = pd.read_csv('/Users/gabrielabib/Desktop/NOVA_SBE/PBL/BERT/protocols.csv')\n",
    "protocols_acceptance = protocols[['Acceptance']]\n",
    "protocols_refusal = protocols[['Refusal']]\n",
    "protocols_acceptance.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nova' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nova ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# # 1. Data cleaning\n",
    "lower_text(protocols_acceptance,'Acceptance', 'Acceptance')\n",
    "lower_text(protocols_refusal,'Refusal', 'Refusal')\n",
    "remove_stop_words(protocols_acceptance,'Acceptance', 'Acceptance')\n",
    "remove_stop_words(protocols_refusal,'Refusal', 'Refusal')\n",
    "spacy_lemmatizer(protocols_acceptance,'Acceptance', 'Acceptance')\n",
    "spacy_lemmatizer(protocols_refusal,'Refusal', 'Refusal')\n",
    "\n",
    "protocols_acceptance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nova' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nova ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "acceptance_list = protocols_acceptance['Acceptance'].tolist()\n",
    "acceptance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nova' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nova ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "[acceptance_list[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nova' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nova ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "alertP1 = connection(\"credentials.csv\")\n",
    "alertP1 = alertP1[alertP1['text_length']>0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nova' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nova ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "alertP1_simplified = alertP1[['COD_REFERENCIA', 'Texto']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nova' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nova ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "alertP1_simplified.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nova' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nova ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "lower_text(alertP1_simplified,'Texto', 'Texto')\n",
    "remove_stop_words(alertP1_simplified,'Texto', 'Texto_Stop_Words')\n",
    "spacy_lemmatizer(alertP1_simplified,'Texto_Stop_Words', 'text_lemmatized')\n",
    "alertP1_simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nova' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nova ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nova' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nova ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nova' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nova ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# # THIS SHOULD BE DELETED\n",
    "\n",
    "# alertP1_simplified = alertP1_simplified[:100]\n",
    "# alertP1_simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nova' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nova ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# referrals = alertP1_simplified['text_lemmatized'].tolist()\n",
    "\n",
    "\n",
    "# for index, protocol in enumerate(acceptance_list):\n",
    "\n",
    "#     sentences = [protocol] + referrals\n",
    "\n",
    "#     # the following line is only necessary if you want to limit the number of sentences. MUST BE COMMENTED OUT TO RUN ON FULL DATASET\n",
    "#     # sentences = sentences[:100]\n",
    "\n",
    "#     # initialize dictionary to store tokenized sentences\n",
    "#     tokens = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "#     for sentence in sentences:\n",
    "#         # encode each sentence and append to dictionary\n",
    "#         new_tokens = tokenizer.encode_plus(sentence, max_length=128,\n",
    "#                                         truncation=True, padding='max_length',\n",
    "#                                         return_tensors='pt')\n",
    "#         tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
    "#         tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
    "\n",
    "#     # reformat list of tensors into single tensor\n",
    "#     tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
    "#     tokens['attention_mask'] = torch.stack(tokens['attention_mask'])\n",
    "        \n",
    "#     outputs = model(**tokens)\n",
    "#     outputs.keys()\n",
    "\n",
    "#     embeddings = outputs.last_hidden_state\n",
    "#     # embeddings\n",
    "\n",
    "#     attention_mask = tokens['attention_mask']\n",
    "#     # attention_mask.shape\n",
    "#     mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "#     # mask.shape\n",
    "\n",
    "#     masked_embeddings = embeddings * mask\n",
    "#     # masked_embeddings.shape\n",
    "\n",
    "#     summed = torch.sum(masked_embeddings, 1)\n",
    "#     # summed.shape\n",
    "\n",
    "#     summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "#     summed_mask.shape\n",
    "\n",
    "#     mean_pooled = summed / summed_mask\n",
    "\n",
    "#     # convert from PyTorch tensor to numpy array\n",
    "#     mean_pooled = mean_pooled.detach().numpy()\n",
    "\n",
    "#     # calculate\n",
    "#     results = cosine_similarity(\n",
    "#         [mean_pooled[0]],\n",
    "#         mean_pooled[1:]\n",
    "#     )\n",
    "\n",
    "#     alertP1_simplified[index] = results[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nova' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nova ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "referrals = alertP1_simplified['text_lemmatized'].tolist()\n",
    "\n",
    "\n",
    "protocol = acceptance_list[0]\n",
    "\n",
    "sentences = [protocol] + referrals\n",
    "\n",
    "# the following line is only necessary if you want to limit the number of sentences. MUST BE COMMENTED OUT TO RUN ON FULL DATASET\n",
    "# sentences = sentences[:100]\n",
    "\n",
    "# initialize dictionary to store tokenized sentences\n",
    "tokens = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "for sentence in sentences:\n",
    "    # encode each sentence and append to dictionary\n",
    "    new_tokens = tokenizer.encode_plus(sentence, max_length=128,\n",
    "                                    truncation=True, padding='max_length',\n",
    "                                    return_tensors='pt')\n",
    "    tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
    "    tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
    "\n",
    "# reformat list of tensors into single tensor\n",
    "tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
    "tokens['attention_mask'] = torch.stack(tokens['attention_mask'])\n",
    "    \n",
    "outputs = model(**tokens)\n",
    "outputs.keys()\n",
    "\n",
    "embeddings = outputs.last_hidden_state\n",
    "# embeddings\n",
    "\n",
    "attention_mask = tokens['attention_mask']\n",
    "# attention_mask.shape\n",
    "mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "# mask.shape\n",
    "\n",
    "masked_embeddings = embeddings * mask\n",
    "# masked_embeddings.shape\n",
    "\n",
    "summed = torch.sum(masked_embeddings, 1)\n",
    "# summed.shape\n",
    "\n",
    "summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "summed_mask.shape\n",
    "\n",
    "mean_pooled = summed / summed_mask\n",
    "\n",
    "# convert from PyTorch tensor to numpy array\n",
    "mean_pooled = mean_pooled.detach().numpy()\n",
    "\n",
    "# calculate\n",
    "results = cosine_similarity(\n",
    "    [mean_pooled[0]],\n",
    "    mean_pooled[1:]\n",
    ")\n",
    "\n",
    "alertP1_simplified['0'] = results[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nova' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nova ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nova' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nova ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nova",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
