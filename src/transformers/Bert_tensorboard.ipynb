{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables: [(bytearray(b'ConsultaUrgencia_doentespedidosconsultaNeurologia2012'),), (bytearray(b'consultaneurologia2012'),), (bytearray(b'consultaneurologia201216anon_true'),), (bytearray(b'hgo_data_032023'),)]\n",
      "308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguelcosta/Desktop/PBL-HGO/src/db/connection.py:30: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  alertP1 = pd.read_sql(\"\"\"SELECT * FROM consultaneurologia201216anon_true\"\"\",mydb)\n",
      "/home/miguelcosta/Desktop/PBL-HGO/src/pre_processing/data_cleaning.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  alertP1['PROVENIENCIA'][alertP1['PROVENIENCIA']=='']='unknown'\n",
      "/home/miguelcosta/Desktop/PBL-HGO/src/pre_processing/data_cleaning.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  alertP1['CTH_PRIOR'][alertP1['CTH_PRIOR']=='']='unknown'\n",
      "/home/miguelcosta/Desktop/PBL-HGO/src/pre_processing/data_cleaning.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  alertP1['COD_UNID_SAUDE_PROV'][alertP1['COD_UNID_SAUDE_PROV']==3151401]=3151400\n",
      "/home/miguelcosta/Desktop/PBL-HGO/src/pre_processing/data_cleaning.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  alertP1['COD_UNID_SAUDE_PROV'][alertP1['COD_UNID_SAUDE_PROV']==3150503]=3150572\n",
      "/home/miguelcosta/Desktop/PBL-HGO/src/pre_processing/data_cleaning.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  alertP1['COD_UNID_SAUDE_PROV'][alertP1['COD_UNID_SAUDE_PROV']==3151603]=3151671\n",
      "/home/miguelcosta/Desktop/PBL-HGO/src/pre_processing/data_cleaning.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  alertP1['COD_UNID_SAUDE_PROV'][alertP1['COD_UNID_SAUDE_PROV']==3152101]=3152100\n",
      "/home/miguelcosta/Desktop/PBL-HGO/src/pre_processing/data_cleaning.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  alertP1['PROVENIENCIA'][alertP1['COD_UNID_SAUDE_PROV']==3150371]='CTH'\n",
      "/home/miguelcosta/Desktop/PBL-HGO/src/pre_processing/features_creation/features_creation_baseline.py:40: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'accepted before' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  alertP1.loc[alertP1['before_accepted']==1, 'before_accepted'] = 'accepted before'\n",
      "/home/miguelcosta/Desktop/PBL-HGO/src/pre_processing/features_creation/features_creation_baseline.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  alertP1['step'][alertP1['step']>=3]='3+'\n",
      "/home/miguelcosta/Desktop/PBL-HGO/src/pre_processing/features_creation/features_creation_baseline.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '3+' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  alertP1['step'][alertP1['step']>=3]='3+'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/miguelcosta/Desktop/PBL-HGO')\n",
    "import pandas as pd\n",
    "# import pandasql as ps\n",
    "#import matplotlib.pyplot as plt\n",
    "# Import argsort\n",
    "from numpy import argsort\n",
    "from src.db.connection import *\n",
    "from src.modelling_pipelines.final_model_pipeline import *\n",
    "from src.modelling_pipelines.modelling_functions.xgboost import *\n",
    "alertP1=connection(\"credentials.txt\")\n",
    "data = pre_process(alertP1)\n",
    "\n",
    "X = data.Texto\n",
    "y = data.result # Target variable\n",
    "\n",
    "# split X and y into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=16,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "\n",
    "def train_biobert_with_dimension_reduction(X_train, y_train, epochs=10, lr=1e-5, batch_size=64, n_components=64):\n",
    "    # Load BioBERTpt model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('pucpr/biobertpt-all')\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('pucpr/biobertpt-all', num_labels=2, output_attentions=True)\n",
    "\n",
    "    # Tokenize and encode sequences in the training set\n",
    "    tokens_train = tokenizer.batch_encode_plus(\n",
    "        X_train.tolist(),\n",
    "        max_length = 200,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "    # Convert integer sequences to tensors\n",
    "    train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "    train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "    train_y = torch.tensor(y_train.tolist())\n",
    "\n",
    "    # Create data loader for training data\n",
    "    train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Specify loss function (Cross-Entropy)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # Train model\n",
    "    writer = SummaryWriter()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for step,batch in enumerate(train_dataloader):\n",
    "            batch = [r.to(device) for r in batch]\n",
    "            sent_id, mask, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(sent_id, mask, labels=labels)  # Pass labels to the model\n",
    "            loss = outputs.loss  # Get the loss directly from the model's outputs\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Extract embeddings for dimensionality reduction\n",
    "            with torch.no_grad():\n",
    "                output = model.base_model(sent_id, mask).last_hidden_state\n",
    "                output = output.mean(dim=1)  # Average over tokens in each sequence\n",
    "\n",
    "                # Fit PCA on the embeddings\n",
    "                U, S, V = torch.pca_lowrank(output, q=n_components)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # ... (same as before)\n",
    "\n",
    "        # ... (same as before)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    return model, V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at pucpr/biobertpt-all and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/miguelcosta/.conda/envs/aequitas/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/miguelcosta/.conda/envs/aequitas/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "q(=64) must be non-negative integer and not greater than min(m, n)=7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model, V \u001b[39m=\u001b[39m train_biobert_with_dimension_reduction(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# Average over tokens in each sequence\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# Fit PCA on the embeddings\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m U, S, V \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mpca_lowrank(output, q\u001b[39m=\u001b[39;49mn_components)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.conda/envs/aequitas/lib/python3.11/site-packages/torch/_lowrank.py:265\u001b[0m, in \u001b[0;36mpca_lowrank\u001b[0;34m(A, q, center, niter)\u001b[0m\n\u001b[1;32m    263\u001b[0m     q \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(\u001b[39m6\u001b[39m, m, n)\n\u001b[1;32m    264\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m (q \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m q \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(m, n)):\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    266\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mq(=\u001b[39m\u001b[39m{\u001b[39;00mq\u001b[39m}\u001b[39;00m\u001b[39m) must be non-negative integer and not greater than min(m, n)=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mmin\u001b[39m(m,\u001b[39m \u001b[39mn)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    268\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (niter \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m    269\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mniter(=\u001b[39m\u001b[39m{\u001b[39;00mniter\u001b[39m}\u001b[39;00m\u001b[39m) must be non-negative integer\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: q(=64) must be non-negative integer and not greater than min(m, n)=7"
     ]
    }
   ],
   "source": [
    "model, V = train_biobert_with_dimension_reduction(X_train, y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Extract embeddings for dimensionality reduction\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mbase_model(sent_id, mask)\u001b[39m.\u001b[39mlast_hidden_state\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# Average over tokens in each sequence\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# Reshape output to 2D (sequences, embedding_dim)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sent_id' is not defined"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=32)\n",
    "# Extract embeddings for dimensionality reduction\n",
    "with torch.no_grad():\n",
    "    output = model.base_model(sent_id, mask).last_hidden_state\n",
    "    output = output.mean(dim=1)  # Average over tokens in each sequence\n",
    "    # Reshape output to 2D (sequences, embedding_dim)\n",
    "    output_2d = output.view(output.shape[0], -1)\n",
    "    # Fit PCA on the embeddings\n",
    "    pca.fit(output.cpu().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at pucpr/biobertpt-all and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/miguelcosta/.conda/envs/aequitas/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/miguelcosta/.conda/envs/aequitas/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_components=64 must be between 0 and min(n_samples, n_features)=7 with svd_solver='full'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model, pca \u001b[39m=\u001b[39m train_biobert_with_dimension_reduction(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# Average over tokens in each sequence\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39m# Fit PCA on the embeddings\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m     pca\u001b[39m.\u001b[39;49mfit(output\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.conda/envs/aequitas/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/aequitas/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:434\u001b[0m, in \u001b[0;36mPCA.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    418\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit the model with X.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \n\u001b[1;32m    420\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[39m        Returns the instance itself.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 434\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X)\n\u001b[1;32m    435\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/aequitas/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:510\u001b[0m, in \u001b[0;36mPCA._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[39m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_svd_solver \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfull\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 510\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_full(X, n_components)\n\u001b[1;32m    511\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_svd_solver \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39marpack\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrandomized\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_truncated(X, n_components, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_svd_solver)\n",
      "File \u001b[0;32m~/.conda/envs/aequitas/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:524\u001b[0m, in \u001b[0;36mPCA._fit_full\u001b[0;34m(self, X, n_components)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    521\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mn_components=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmle\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is only supported if n_samples >= n_features\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m         )\n\u001b[1;32m    523\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m n_components \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(n_samples, n_features):\n\u001b[0;32m--> 524\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    525\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mn_components=\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m must be between 0 and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    526\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmin(n_samples, n_features)=\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msvd_solver=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfull\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (n_components, \u001b[39mmin\u001b[39m(n_samples, n_features))\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    530\u001b[0m \u001b[39m# Center data\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: n_components=64 must be between 0 and min(n_samples, n_features)=7 with svd_solver='full'"
     ]
    }
   ],
   "source": [
    "model, pca = train_biobert_with_dimension_reduction(X_train, y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(model, pca, X_test, batch_size=32):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('pucpr/biobertpt-all')\n",
    "    \n",
    "    tokens_test = tokenizer.batch_encode_plus(\n",
    "        X_test.tolist(),\n",
    "        max_length=200,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "    test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "    test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "\n",
    "    test_data = TensorDataset(test_seq, test_mask)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for batch in test_dataloader:\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.base_model(sent_id, mask).last_hidden_state\n",
    "            output = output.mean(dim=1)  # Average over tokens in each sequence\n",
    "\n",
    "            reduced_embeddings_test = pca.transform(output.cpu().numpy())\n",
    "\n",
    "            outputs = model(reduced_embeddings_test)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "\n",
    "            predictions.append(logits)\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguelcosta/.conda/envs/aequitas/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-10-10 14:57:41.476866: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-10 14:57:41.515557: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-10 14:57:41.515596: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-10 14:57:41.515616: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-10 14:57:41.522123: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-10 14:57:42.379768: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.decomposition import PCA  # Import PCA from scikit-learn\n",
    "\n",
    "def train_biobert_with_dimension_reduction(X_train, y_train, epochs=10, lr=1e-5, batch_size=32, n_components=64):\n",
    "    # Load BioBERTpt model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('pucpr/biobertpt-all')\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('pucpr/biobertpt-all', num_labels=2, output_attentions=True)\n",
    "\n",
    "    # Tokenize and encode sequences in the training set\n",
    "    tokens_train = tokenizer.batch_encode_plus(\n",
    "        X_train.tolist(),\n",
    "        max_length=200,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "    # Convert integer sequences to tensors\n",
    "    train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "    train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "    train_y = torch.tensor(y_train.tolist())\n",
    "\n",
    "    # Create data loader for training data\n",
    "    train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Specify loss function (Cross-Entropy)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # Initialize PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    \n",
    "    # Train model\n",
    "    writer = SummaryWriter()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        reduced_embeddings = []  # Initialize a list to store reduced embeddings\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch = [r.to(device) for r in batch]\n",
    "            sent_id, mask, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(sent_id, mask, labels=labels)  # Pass labels to the model\n",
    "            loss = outputs.loss  # Get the loss directly from the model's outputs\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate and log the average loss for the current step\n",
    "            avg_train_loss = total_loss / (step + 1)  # Step starts from 0\n",
    "            writer.add_scalar('Loss/train (step)', avg_train_loss, step + epoch * len(train_dataloader))\n",
    "\n",
    "            # Extract embeddings for dimensionality reduction\n",
    "            with torch.no_grad():\n",
    "                output = model.base_model(sent_id, mask).last_hidden_state\n",
    "                output = output.mean(dim=1)  # Average over tokens in each sequence\n",
    "\n",
    "                # Fit PCA on the embeddings\n",
    "                pca.fit(output.cpu().numpy())\n",
    "\n",
    "\n",
    "        # Concatenate reduced embeddings into a single tensor\n",
    "        reduced_embeddings = torch.cat(reduced_embeddings, dim=0)\n",
    "\n",
    "        # Perform PCA dimensionality reduction on the embeddings\n",
    "        pca = PCA(n_components=n_components)\n",
    "        reduced_embeddings = pca.fit_transform(reduced_embeddings.cpu().numpy())\n",
    "\n",
    "        # Metrics for TensorBoard\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        writer.add_scalar('Loss/train (epoch)', avg_train_loss, epoch)\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            writer.add_scalar('Learning_Rate', param_group['lr'], epoch)\n",
    "\n",
    "        # Log model parameters and gradients\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     writer.add_histogram('Parameters/' + name, param.data.cpu().numpy(), epoch)\n",
    "        #     if param.grad is not None:\n",
    "        #         writer.add_histogram('Gradients/' + name, param.grad.data.cpu().numpy(), epoch)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    # Return the trained model and reduced embeddings\n",
    "    return model, reduced_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at pucpr/biobertpt-all and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/miguelcosta/.conda/envs/aequitas/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/miguelcosta/.conda/envs/aequitas/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/miguelcosta/.conda/envs/aequitas/lib/python3.11/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "model, reduced_embeddings = train_biobert_with_dimension_reduction(X_train, y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, pca, X_test, batch_size=32):\n",
    "    # Tokenize and encode sequences in the test set\n",
    "    tokenizer = AutoTokenizer.from_pretrained('pucpr/biobertpt-all')\n",
    "    tokens_test = tokenizer.batch_encode_plus(\n",
    "        X_test.tolist(),\n",
    "        max_length=200,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "    # Convert integer sequences to tensors\n",
    "    test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "    test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "\n",
    "    # Create data loader for test data\n",
    "    test_data = TensorDataset(test_seq, test_mask)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Initialize lists to store predictions and true labels\n",
    "    predictions = []\n",
    "    \n",
    "    # Predict labels for batches in the test set\n",
    "    model.eval()\n",
    "    for batch in test_dataloader:\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.base_model(sent_id, mask).last_hidden_state\n",
    "            output = output.mean(dim=1)  # Average over tokens in each sequence\n",
    "\n",
    "            # Apply PCA transformation on the embeddings\n",
    "            reduced_embeddings_test = pca.transform(output.cpu().numpy())\n",
    "\n",
    "            outputs = model(reduced_embeddings_test)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        # Store predictions and true labels\n",
    "        predictions.append(logits)\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m predictions \u001b[39m=\u001b[39m predict(model, pca, X_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pca' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = predict(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at pucpr/biobertpt-all and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/miguelcosta/.conda/envs/aequitas/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, trainer\n",
    "import torch\n",
    "\n",
    "# Load BioBERTpt model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('pucpr/biobertpt-all')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('pucpr/biobertpt-all', num_labels=2, output_attentions=True)\n",
    "\n",
    "# Tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    X_train.tolist(),\n",
    "    max_length = 200,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biobertpt-finetuned-classification\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_output_dir = \"biobertpt-finetuned-classification\"\n",
    "print(model_output_dir)\n",
    "\n",
    "# Start TensorBoard before training to monitor it in progress\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir '{model_output_dir}'/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# output_dir: directory where the model checkpoints will be saved.\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     output_dir\u001b[39m=\u001b[39;49mmodel_output_dir,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# evaluation_strategy (default \"no\"):\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# Possible values are:\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# \"no\": No evaluation is done during training.\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# \"steps\": Evaluation is done (and logged) every eval_steps.\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# \"epoch\": Evaluation is done at the end of each epoch.\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     evaluation_strategy\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msteps\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# eval_steps: Number of update steps between two evaluations if\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# evaluation_strategy=\"steps\". Will default to the same value as\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# logging_steps if not set.\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     eval_steps\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# logging_strategy (default: \"steps\"): The logging strategy to adopt during\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# training (used to log training loss for example). Possible values are:\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# \"no\": No logging is done during training.\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# \"epoch\": Logging is done at the end of each epoch.\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# \"steps\": Logging is done every logging_steps.\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     logging_strategy\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msteps\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# logging_steps (default 500): Number of update steps between two logs if\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# logging_strategy=\"steps\".\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     logging_steps\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# save_strategy (default \"steps\"):\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m# The checkpoint save strategy to adopt during training. Possible values are:\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m# \"no\": No save is done during training.\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39m# \"epoch\": Save is done at the end of each epoch.\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39m# \"steps\": Save is done every save_steps (default 500).\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     save_strategy\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msteps\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39m# save_steps (default: 500): Number of updates steps before two checkpoint\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39m# saves if save_strategy=\"steps\".\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     save_steps\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39m# learning_rate (default 5e-5): The initial learning rate for AdamW optimizer.\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39m# Adam algorithm with weight decay fix as introduced in the paper\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39m# Decoupled Weight Decay Regularization.\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39m2e-5\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39m# per_device_train_batch_size: The batch size per GPU/TPU core/CPU for training.\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39m# per_device_eval_batch_size: The batch size per GPU/TPU core/CPU for evaluation.\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     per_device_eval_batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m# num_train_epochs (default 3.0): Total number of training epochs to perform\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39m# (if not an integer, will perform the decimal part percents of the last epoch\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m# before stopping training).\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39m# load_best_model_at_end (default False): Whether or not to load the best model\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39m# found during training at the end of training.\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     load_best_model_at_end\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39m# metric_for_best_model:\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39m# Use in conjunction with load_best_model_at_end to specify the metric to use\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39m# to compare two different models. Must be the name of a metric returned by\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39m# the evaluation with or without the prefix \"eval_\".\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m     metric_for_best_model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39m# report_to:\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39m# The list of integrations to report the results and logs to. Supported\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39m# platforms are \"azure_ml\", \"comet_ml\", \"mlflow\", \"tensorboard\" and \"wandb\".\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39m# Use \"all\" to report to all integrations installed, \"none\" for no integrations.\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     report_to\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtensorboard\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.20.20.4/home/miguelcosta/Desktop/PBL-HGO/src/transformers/Bert_tensorboard.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m )\n",
      "File \u001b[0;32m<string>:115\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, include_tokens_per_second)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/aequitas/lib/python3.11/site-packages/transformers/training_args.py:1436\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1430\u001b[0m     \u001b[39mif\u001b[39;00m version\u001b[39m.\u001b[39mparse(version\u001b[39m.\u001b[39mparse(torch\u001b[39m.\u001b[39m__version__)\u001b[39m.\u001b[39mbase_version) \u001b[39m==\u001b[39m version\u001b[39m.\u001b[39mparse(\u001b[39m\"\u001b[39m\u001b[39m2.0.0\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16:\n\u001b[1;32m   1431\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1433\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1434\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1435\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[0;32m-> 1436\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1437\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1438\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1439\u001b[0m     \u001b[39mand\u001b[39;00m (get_xla_device_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGPU\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1440\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16_full_eval)\n\u001b[1;32m   1441\u001b[0m ):\n\u001b[1;32m   1442\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1443\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1444\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1445\u001b[0m     )\n\u001b[1;32m   1447\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1448\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1449\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16_full_eval)\n\u001b[1;32m   1457\u001b[0m ):\n",
      "File \u001b[0;32m~/.conda/envs/aequitas/lib/python3.11/site-packages/transformers/training_args.py:1901\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1897\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1898\u001b[0m \u001b[39mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   1899\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m requires_backends(\u001b[39mself\u001b[39m, [\u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m-> 1901\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_devices\n",
      "File \u001b[0;32m~/.conda/envs/aequitas/lib/python3.11/site-packages/transformers/utils/generic.py:54\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     52\u001b[0m cached \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, attr, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m cached \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     cached \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfget(obj)\n\u001b[1;32m     55\u001b[0m     \u001b[39msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/.conda/envs/aequitas/lib/python3.11/site-packages/transformers/training_args.py:1801\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   1800\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_accelerate_available(min_version\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m0.20.1\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1801\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m   1802\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1803\u001b[0m         )\n\u001b[1;32m   1804\u001b[0m     AcceleratorState\u001b[39m.\u001b[39m_reset_state(reset_partial_state\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1805\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistributed_state \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    # output_dir: directory where the model checkpoints will be saved.\n",
    "    output_dir=model_output_dir,\n",
    "    # evaluation_strategy (default \"no\"):\n",
    "    # Possible values are:\n",
    "    # \"no\": No evaluation is done during training.\n",
    "    # \"steps\": Evaluation is done (and logged) every eval_steps.\n",
    "    # \"epoch\": Evaluation is done at the end of each epoch.\n",
    "    evaluation_strategy=\"steps\",\n",
    "    # eval_steps: Number of update steps between two evaluations if\n",
    "    # evaluation_strategy=\"steps\". Will default to the same value as\n",
    "    # logging_steps if not set.\n",
    "    eval_steps=50,\n",
    "    # logging_strategy (default: \"steps\"): The logging strategy to adopt during\n",
    "    # training (used to log training loss for example). Possible values are:\n",
    "    # \"no\": No logging is done during training.\n",
    "    # \"epoch\": Logging is done at the end of each epoch.\n",
    "    # \"steps\": Logging is done every logging_steps.\n",
    "    logging_strategy=\"steps\",\n",
    "    # logging_steps (default 500): Number of update steps between two logs if\n",
    "    # logging_strategy=\"steps\".\n",
    "    logging_steps=50,\n",
    "    # save_strategy (default \"steps\"):\n",
    "    # The checkpoint save strategy to adopt during training. Possible values are:\n",
    "    # \"no\": No save is done during training.\n",
    "    # \"epoch\": Save is done at the end of each epoch.\n",
    "    # \"steps\": Save is done every save_steps (default 500).\n",
    "    save_strategy=\"steps\",\n",
    "    # save_steps (default: 500): Number of updates steps before two checkpoint\n",
    "    # saves if save_strategy=\"steps\".\n",
    "    save_steps=200,\n",
    "    # learning_rate (default 5e-5): The initial learning rate for AdamW optimizer.\n",
    "    # Adam algorithm with weight decay fix as introduced in the paper\n",
    "    # Decoupled Weight Decay Regularization.\n",
    "    learning_rate=2e-5,\n",
    "    # per_device_train_batch_size: The batch size per GPU/TPU core/CPU for training.\n",
    "    per_device_train_batch_size=16,\n",
    "    # per_device_eval_batch_size: The batch size per GPU/TPU core/CPU for evaluation.\n",
    "    per_device_eval_batch_size=16,\n",
    "    # num_train_epochs (default 3.0): Total number of training epochs to perform\n",
    "    # (if not an integer, will perform the decimal part percents of the last epoch\n",
    "    # before stopping training).\n",
    "    num_train_epochs=1,\n",
    "    # load_best_model_at_end (default False): Whether or not to load the best model\n",
    "    # found during training at the end of training.\n",
    "    load_best_model_at_end=True,\n",
    "    # metric_for_best_model:\n",
    "    # Use in conjunction with load_best_model_at_end to specify the metric to use\n",
    "    # to compare two different models. Must be the name of a metric returned by\n",
    "    # the evaluation with or without the prefix \"eval_\".\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    # report_to:\n",
    "    # The list of integrations to report the results and logs to. Supported\n",
    "    # platforms are \"azure_ml\", \"comet_ml\", \"mlflow\", \"tensorboard\" and \"wandb\".\n",
    "    # Use \"all\" to report to all integrations installed, \"none\" for no integrations.\n",
    "    report_to=\"tensorboard\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns an untrained model to be trained\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_checkpoint,\n",
    "                                                              num_labels=2)\n",
    "\n",
    "# Function that will be called at the end of each evaluation phase on the whole\n",
    "# arrays of predictions/labels to produce metrics.\n",
    "def compute_metrics(eval_pred):\n",
    "    # Predictions and labels are grouped in a namedtuple called EvalPrediction\n",
    "    predictions, labels = eval_pred\n",
    "    # Get the index with the highest prediction score (i.e. the predicted labels)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    # Compare the predicted labels with the reference labels\n",
    "    results =  metric.compute(predictions=predictions, references=labels)\n",
    "    # results: a dictionary with string keys (the name of the metric) and float\n",
    "    # values (i.e. the metric values)\n",
    "    return results\n",
    "\n",
    "# Since PyTorch does not provide a training loop, the 🤗 Transformers library\n",
    "# provides a Trainer API that is optimized for 🤗 Transformers models, with a\n",
    "# wide range of training options and with built-in features like logging,\n",
    "# gradient accumulation, and mixed precision.\n",
    "trainer = Trainer(\n",
    "    # Function that returns the model to train. It's useful to use a function\n",
    "    # instead of directly the model to make sure that we are always training\n",
    "    # an untrained model from scratch.\n",
    "    model_init=model_init,\n",
    "    # The training arguments.\n",
    "    args=args,\n",
    "    # The training dataset.\n",
    "    train_dataset=splitted_datasets_encoded[\"train\"],\n",
    "    # The evaluation dataset. We use a small subset of the validation set\n",
    "    # composed of 150 samples to speed up computations...\n",
    "    eval_dataset=splitted_datasets_encoded[\"test\"].shuffle(42).select(range(150)),\n",
    "    # Even though the training set and evaluation set are already tokenized, the\n",
    "    # tokenizer is needed to pad the \"input_ids\" and \"attention_mask\" tensors\n",
    "    # to the length managed by the model. It does so one batch at a time, to\n",
    "    # use less memory as possible.\n",
    "    tokenizer=tokenizer,\n",
    "    # Function that will be called at the end of each evaluation phase on the whole\n",
    "    # arrays of predictions/labels to produce metrics.\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# ... train the model!\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aequitas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
