{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "883aa1a1",
   "metadata": {},
   "source": [
    "# TF-IDF Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95d1c94",
   "metadata": {},
   "source": [
    "TF-IDF (term frequency-inverse document frequency) analysis is a statistical technique used in natural language processing and information retrieval to determine the importance of a word in a document or corpus. It is a way to measure how relevant a word is to a document in a collection of documents.\n",
    "\n",
    "TF-IDF analysis assigns a weight to each word in a document based on how frequently it appears in the document (term frequency) and how rare it is in the entire corpus (inverse document frequency). The weight assigned to a word increases proportionally with its frequency in the document, but is offset by the rarity of the word in the corpus. This means that words that appear frequently in a document but also appear frequently in many other documents in the corpus are given a lower weight, while words that appear less frequently in the corpus but frequently in a particular document are given a higher weight.\n",
    "\n",
    "The output of TF-IDF analysis is a numerical representation of each document that captures the importance of each word in that document. This can be used for various tasks such as text classification, clustering, and information retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ffe588",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Connect to Database ](#Connect-to-database)\n",
    "* [Import Datasets](#Import-Dataset)\n",
    "* [Remove Stopwords](#Remove-stopwords)\n",
    "* [Lemmatization](#Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a710249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import Timestamp\n",
    "from IPython.display import display\n",
    "from Functions.connection.connection import *\n",
    "from Functions.AlertP1.data_cleaning import *\n",
    "from Functions.AlertP1.features import *\n",
    "from Functions.analysis.step_analysis import *\n",
    "from Functions.AlertP1.dummy_features import *\n",
    "from Functions.Models.decision_tree import *\n",
    "from Functions.Models.Logistic_regression import *\n",
    "from Functions.Models.evaluation import *\n",
    "import spacy\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from numpy import argsort\n",
    "from Functions.AlertP1.data_cleaning import *\n",
    "from Functions.AlertP1.features import *\n",
    "from Functions.analysis.step_analysis import *\n",
    "from Functions.AlertP1.dummy_features import *\n",
    "from Functions.Models.evaluation import *\n",
    "from Functions.NLP.alertp1_nlp import *\n",
    "from Functions.NLP.data_with_nlp import *\n",
    "from Functions.Pipelines import pipeline_NLP as NLP\n",
    "#from Functions.Pipelines import *\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import nltk\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b973cc15",
   "metadata": {},
   "source": [
    "## Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee28e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "#creds = [\"username\",\"password\",\"juliehaegh\",\"ninG20&19rea\",\"3306\"] \n",
    "creds = [\"juliehaegh\",\"ninG20&19rea\",\"172.20.20.4\",\"hgo\",3306]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11097233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables: [('ConsultaUrgencia_doentespedidosconsultaNeurologia2012',), ('consultaneurologia2012',), ('consultaneurologia201216anon_true',), ('hgo_data_032023',)]\n",
      "275\n"
     ]
    }
   ],
   "source": [
    "#Connection to the database\n",
    "host = creds[2]\n",
    "user = creds[0]\n",
    "password = creds[1]\n",
    "database = creds[3]\n",
    "port = creds[4]\n",
    "mydb = mysql.connector.connect(host=host, user=user, database=database, port=port, password=password, auth_plugin='mysql_native_password')\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "#Safecheck to guarantee that the connection worked\n",
    "mycursor.execute('SHOW TABLES;')\n",
    "print(f\"Tables: {mycursor.fetchall()}\")\n",
    "print(mydb.connection_id) #it'll give connection_id,if got connected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0b9ef7",
   "metadata": {},
   "source": [
    "## Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a031a49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s4/b954cnz56f51x2w2tygb_fvc0000gn/T/ipykernel_35452/676277896.py:2: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  SClinic = pd.read_sql(\"\"\"SELECT * FROM ConsultaUrgencia_doentespedidosconsultaNeurologia2012\"\"\",mydb)\n",
      "/var/folders/s4/b954cnz56f51x2w2tygb_fvc0000gn/T/ipykernel_35452/676277896.py:5: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  AlertP1 = pd.read_sql(\"\"\"SELECT * FROM consultaneurologia201216anon_true\"\"\",mydb)\n"
     ]
    }
   ],
   "source": [
    "# Import Alert P1 dataset\n",
    "SClinic = pd.read_sql(\"\"\"SELECT * FROM ConsultaUrgencia_doentespedidosconsultaNeurologia2012\"\"\",mydb)\n",
    "\n",
    "# Import SClinic\n",
    "AlertP1 = pd.read_sql(\"\"\"SELECT * FROM consultaneurologia201216anon_true\"\"\",mydb)\n",
    "\n",
    "# Replace all NaN with 0\n",
    "AlertP1 = AlertP1.fillna(0)\n",
    "\n",
    "# Add result column\n",
    "AlertP1['result'] = ['Accepted' if x in [0,14,25,20,53,8,12,12] else 'Refused' for x in AlertP1['COD_MOTIVO_RECUSA']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0215a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliehaegh/Documents/NovaSBE/PBL/Functions/AlertP1/data_cleaning.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  alertP1['PROVENIENCIA'][alertP1['PROVENIENCIA']=='']='unknown'\n",
      "/Users/juliehaegh/Documents/NovaSBE/PBL/Functions/AlertP1/data_cleaning.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  alertP1['CTH_PRIOR'][alertP1['CTH_PRIOR']=='']='unknown'\n",
      "/Users/juliehaegh/Documents/NovaSBE/PBL/Functions/AlertP1/data_cleaning.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  alertP1['COD_UNID_SAUDE_PROV'][alertP1['COD_UNID_SAUDE_PROV']==3151401]=3151400\n",
      "/Users/juliehaegh/Documents/NovaSBE/PBL/Functions/AlertP1/data_cleaning.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  alertP1['COD_UNID_SAUDE_PROV'][alertP1['COD_UNID_SAUDE_PROV']==3150503]=3150572\n",
      "/Users/juliehaegh/Documents/NovaSBE/PBL/Functions/AlertP1/data_cleaning.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  alertP1['COD_UNID_SAUDE_PROV'][alertP1['COD_UNID_SAUDE_PROV']==3151603]=3151671\n",
      "/Users/juliehaegh/Documents/NovaSBE/PBL/Functions/AlertP1/data_cleaning.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  alertP1['COD_UNID_SAUDE_PROV'][alertP1['COD_UNID_SAUDE_PROV']==3152101]=3152100\n",
      "/Users/juliehaegh/Documents/NovaSBE/PBL/Functions/AlertP1/data_cleaning.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  alertP1['PROVENIENCIA'][alertP1['COD_UNID_SAUDE_PROV']==3150371]='CTH'\n",
      "/Users/juliehaegh/Documents/NovaSBE/PBL/Functions/AlertP1/features.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  alertP1['step'][alertP1['step']>=3]='3+'\n",
      "/Users/juliehaegh/anaconda3/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juliehaegh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/juliehaegh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "data = NLP.pre_process(AlertP1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12d2843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['Texto']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d93ef897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Split data into train and test\n",
    "AlertP1_sorted = data[data['Texto']!=''].sort_values(by='DATA_RECEPCAO')\n",
    "\n",
    "# calculate the index for the split\n",
    "split_index = math.ceil(0.8 * len(AlertP1_sorted))\n",
    "\n",
    "# split the data frame into test and train sets\n",
    "train_set = AlertP1_sorted.iloc[:split_index]\n",
    "test_set = AlertP1_sorted.iloc[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2b7ea14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juliehaegh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/juliehaegh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import librariers \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a088fb",
   "metadata": {},
   "source": [
    "## Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9b4873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of special characters and transform Texto column to Latin words\n",
    "train_set['Texto'] = train_set['Texto'].apply(lambda x: unidecode(x))\n",
    "\n",
    "#The re.sub function is used to substitute all digits (\\d) with an empty string\n",
    "train_set['Texto'] = train_set['Texto'].apply(lambda x: re.sub(r'\\d', '', x))\n",
    "\n",
    "# Remove all names in Texto variable\n",
    "# This function uses a regular expression to find all words in the text that start with a \n",
    "# capital letter (\\b[A-Z][a-z]+\\b), which are assumed to be names\n",
    "text = train_set['Texto'] \n",
    "\n",
    "# remove all hyphens from the text\n",
    "text = text.replace('-', '')\n",
    "\n",
    "def remove_names(text):\n",
    "    # Find all words that start with a capital letter\n",
    "    names = re.findall(r'\\b[A-Z][a-z]+\\b', text)\n",
    "    \n",
    "    # Replace the names with an empty string\n",
    "    for name in names:\n",
    "        text = text.replace(name, '')\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2069e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the text\n",
    "text_list = []\n",
    "\n",
    "# Loop through the 'text' column\n",
    "for text in text.str.lower(): # Transform every word to lower case\n",
    "    text_list.append(text)\n",
    "\n",
    "# Print the list of text\n",
    "#print(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2e7d46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juliehaegh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/juliehaegh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the Portuguese stop words\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Get the Portuguese stop words\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Manually remove stopwords\n",
    "stop_words.update(['-//','.', ',','(',')',':','-','?','+','/',';','2','1','drª','``','','3','desde','anos','doente','consulta','alterações','se',\"''\",'cerca','refere','hgo','utente','vossa','s','...','ainda','c','filha','costa','dr.','pereira','ja','--','p','dr','h','n','>','q','//','..','b','++','%','//','-','+++/','=','+++/'])\n",
    "\n",
    "# Create a new list to store the filtered text\n",
    "filtered_text = []\n",
    "\n",
    "# Loop through the text_list and remove the stop words\n",
    "for text in text_list:\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    filtered_text.append(\" \".join(words))\n",
    "\n",
    "# Print the filtered text\n",
    "#print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b63efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the filtered text as a new column to the dataframe\n",
    "train_set['filtered_text'] = filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95691b55",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e87d5",
   "metadata": {},
   "source": [
    "Lemmatization is a text normalization technique used in Natural Language Processing (NLP), that switches any kind of a word to its base root mode. Lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "449168bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for lemmatization\n",
    "def spacy_lemmatizer(data):\n",
    "    import spacy\n",
    "    import pt_core_news_md\n",
    "    nlp = pt_core_news_md.load()\n",
    "\n",
    "    doclist = list(nlp.pipe(data))\n",
    "\n",
    "    docs=[]\n",
    "    for i, doc in enumerate(doclist):\n",
    "        docs.append(' '.join([listitem.lemma_ for listitem in doc]))\n",
    "        \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c39bfb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list to store the words\n",
    "word_list = []\n",
    "\n",
    "# loop through each row of the \"text_column\" column\n",
    "for index, row in train_set.iterrows():\n",
    "    \n",
    "    # split the text into individual words using whitespace as a delimiter\n",
    "    words = row['filtered_text'].split()\n",
    "    # add the words to the word list\n",
    "    word_list.extend(words)\n",
    "\n",
    "# print the word list\n",
    "#print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2657afaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list to store the words\n",
    "word_list = []\n",
    "\n",
    "# loop through each row of the \"text_column\" column\n",
    "for index, row in train_set.iterrows():\n",
    "    \n",
    "    # split the text into individual words using whitespace as a delimiter\n",
    "    words = row['filtered_text'].split()\n",
    "    \n",
    "    # remove hyphens from the words and add them to the word list\n",
    "    word_list.extend([word.replace('-', '') for word in words])\n",
    "    # remove slash from the words and ass them to the list\n",
    "    word_list.extend([word.replace('/', '') for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56096fb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words after lemmatization: 8101\n",
      "The number of words before lemmatization: 9885\n"
     ]
    }
   ],
   "source": [
    "Lemma = spacy_lemmatizer(word_list) # Call lemmatizer function\n",
    "\n",
    "# print length of word_list and compare the count after doing lemmatization\n",
    "from collections import Counter\n",
    "\n",
    "items = Counter(Lemma).keys()\n",
    "print('The number of words after lemmatization:',len(items))\n",
    "\n",
    "items2 = Counter(word_list).keys()\n",
    "print('The number of words before lemmatization:',len(items2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98955bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>filtered_text</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>idade tremor acentuar membro alto agravamento ...</td>\n",
       "      <td>idade tremor acentuado membros superiores agra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>relatorio clinico</td>\n",
       "      <td>relatorio clinico</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>trazer inumero relatorio ida hgo-su vulvo-vagi...</td>\n",
       "      <td>traz inumeros relatorios idas hgo-su vulvo-vag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>tremor fino ambos mao bocar ha mês deixar cair...</td>\n",
       "      <td>tremor fino ambas maos boca ha meses deixa cai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>pe pendente instalacao ha mês atrofiar muscula...</td>\n",
       "      <td>pe pendente instalacao ha meses atrofia muscul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>dte sii episodio recorrente lipotimer alteraco...</td>\n",
       "      <td>dte sii episodios recorrentes lipotimia altera...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>problema saude resolver demencia se enil / alz...</td>\n",
       "      <td>problema saude resolver demencia senil/alzheim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>antecedente epilepsia + ultimo ano crise frequ...</td>\n",
       "      <td>antecedentes epilepsia +ultimo ano crises freq...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>mulher idade diagnosticos dislipidemia hipertr...</td>\n",
       "      <td>mulher idade diagnosticos dislipidemia hipertr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>crise convulsivo inaugural motora generalizaca...</td>\n",
       "      <td>crise convulsiva inaugural motora generalizaca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1285 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text_lemmatized  \\\n",
       "630   idade tremor acentuar membro alto agravamento ...   \n",
       "1537                                  relatorio clinico   \n",
       "985   trazer inumero relatorio ida hgo-su vulvo-vagi...   \n",
       "1103  tremor fino ambos mao bocar ha mês deixar cair...   \n",
       "752   pe pendente instalacao ha mês atrofiar muscula...   \n",
       "...                                                 ...   \n",
       "471   dte sii episodio recorrente lipotimer alteraco...   \n",
       "1498  problema saude resolver demencia se enil / alz...   \n",
       "244   antecedente epilepsia + ultimo ano crise frequ...   \n",
       "1118  mulher idade diagnosticos dislipidemia hipertr...   \n",
       "629   crise convulsivo inaugural motora generalizaca...   \n",
       "\n",
       "                                          filtered_text  result  \n",
       "630   idade tremor acentuado membros superiores agra...       1  \n",
       "1537                                  relatorio clinico       0  \n",
       "985   traz inumeros relatorios idas hgo-su vulvo-vag...       1  \n",
       "1103  tremor fino ambas maos boca ha meses deixa cai...       1  \n",
       "752   pe pendente instalacao ha meses atrofia muscul...       1  \n",
       "...                                                 ...     ...  \n",
       "471   dte sii episodios recorrentes lipotimia altera...       0  \n",
       "1498  problema saude resolver demencia senil/alzheim...       1  \n",
       "244   antecedentes epilepsia +ultimo ano crises freq...       0  \n",
       "1118  mulher idade diagnosticos dislipidemia hipertr...       1  \n",
       "629   crise convulsiva inaugural motora generalizaca...       1  \n",
       "\n",
       "[1285 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the spacy_lemmatizer function to each row in the 'text' column\n",
    "train_set['text_lemmatized'] = spacy_lemmatizer(train_set['filtered_text'])\n",
    "\n",
    "# drop rows with empty strings\n",
    "train_set_filtered = train_set[['text_lemmatized','filtered_text','result']].replace('', pd.NA).dropna()\n",
    "train_set_filtered = pd.DataFrame(train_set_filtered)\n",
    "train_set_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60f86469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lambda function to apply to the DataFrame\n",
    "train_set_filtered['accepted/rejected'] = train_set_filtered['result'].apply(lambda x: 1 if x == 'Accepted' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58cace48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into train and test sets with 20% test size\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_set_filtered['filtered_text'], train_set_filtered['accepted/rejected'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer object with max_df = 0.8 and min_df = 5\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, min_df=5)\n",
    "\n",
    "# Fit and transform the training data into a sparse matrix of TF-IDF features\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data into a sparse matrix of TF-IDF features\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Use the TF scores for future predictions\n",
    "tf_vectorizer = TfidfVectorizer(use_idf=False, max_df=0.8, min_df=5)\n",
    "X_tf = tf_vectorizer.fit_transform(train_set_filtered['filtered_text'])\n",
    "\n",
    "# Use the TF-IDF scores for logistic regression with other variables\n",
    "X_train_tfidf_lr = pd.concat([pd.DataFrame(X_train_tfidf.toarray()), train_set_filtered[train_set_filtered.index.isin(X_train.index)].drop(['filtered_text', 'accepted/rejected'], axis=1)], axis=1)\n",
    "X_test_tfidf_lr = pd.concat([pd.DataFrame(X_test_tfidf.toarray()), train_set_filtered[train_set_filtered.index.isin(X_test.index)].drop(['filtered_text', 'accepted/rejected'], axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbe2c3e",
   "metadata": {},
   "source": [
    "max_df is a float in the range [0.0, 1.0] or an integer, representing the maximum frequency of a term allowed in the corpus. Terms with a frequency higher than this threshold will be ignored as they are considered too common to be informative. In the code provided, we set max_df=0.8, which means that we are ignoring terms that appear in more than 80% of the documents in the corpus.\n",
    "\n",
    "min_df is also a float in the range [0.0, 1.0] or an integer, representing the minimum frequency of a term allowed in the corpus. Terms with a frequency lower than this threshold will be ignored as they are considered too rare to be informative. In the code provided, we set min_df=5, which means that we are ignoring terms that appear in less than 5 documents in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52e67ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1554"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561748bd",
   "metadata": {},
   "source": [
    "\n",
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62fa274e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 words with the highest scores for the train set:\n",
      "relatorio: 0.0733\n",
      "clinico: 0.0701\n",
      "epilepsia: 0.0313\n",
      "nao: 0.0304\n",
      "alteracoes: 0.0274\n",
      "ha: 0.0256\n",
      "neurologia: 0.0219\n",
      "demencia: 0.0207\n",
      "mg: 0.0190\n",
      "tac: 0.0181\n",
      "se: 0.0180\n",
      "tremor: 0.0175\n",
      "cefaleias: 0.0169\n",
      "quadro: 0.0168\n",
      "avaliacao: 0.0167\n",
      "ce: 0.0165\n",
      "hta: 0.0162\n",
      "memoria: 0.0155\n",
      "agravamento: 0.0152\n",
      "antecedentes: 0.0149\n",
      "\n",
      "Top 20 words with the highest scores for the test set:\n",
      "relatorio: 0.0893\n",
      "clinico: 0.0866\n",
      "nao: 0.0324\n",
      "tremor: 0.0302\n",
      "alteracoes: 0.0292\n",
      "epilepsia: 0.0269\n",
      "ha: 0.0269\n",
      "se: 0.0258\n",
      "memoria: 0.0240\n",
      "demencia: 0.0217\n",
      "tac: 0.0210\n",
      "marcha: 0.0197\n",
      "antecedentes: 0.0188\n",
      "mg: 0.0184\n",
      "ce: 0.0180\n",
      "informacao: 0.0169\n",
      "neurologia: 0.0168\n",
      "hta: 0.0157\n",
      "meses: 0.0157\n",
      "dra: 0.0153\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_set_filtered['filtered_text'], train_set_filtered['result'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the TF-IDF vectorizer object\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, min_df=5)\n",
    "\n",
    "# Fit and transform the vectorizer on the train set\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Extract the top 10 words with the highest scores for the train set\n",
    "train_scores = np.asarray(X_train_tfidf.mean(axis=0)).ravel().tolist()\n",
    "train_features = tfidf_vectorizer.get_feature_names_out()\n",
    "top_train_indices = np.argsort(train_scores)[::-1][:20]\n",
    "top_train_words = [train_features[i] for i in top_train_indices]\n",
    "top_train_scores = [train_scores[i] for i in top_train_indices]\n",
    "\n",
    "# Transform the vectorizer on the test set\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Extract the top 10 words with the highest scores for the test set\n",
    "test_scores = np.asarray(X_test_tfidf.mean(axis=0)).ravel().tolist()\n",
    "test_features = tfidf_vectorizer.get_feature_names_out()\n",
    "top_test_indices = np.argsort(test_scores)[::-1][:20]\n",
    "top_test_words = [test_features[i] for i in top_test_indices]\n",
    "top_test_scores = [test_scores[i] for i in top_test_indices]\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 20 words with the highest scores for the train set:\")\n",
    "for word, score in zip(top_train_words, top_train_scores):\n",
    "    print(f\"{word}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 20 words with the highest scores for the test set:\")\n",
    "for word, score in zip(top_test_words, top_test_scores):\n",
    "    print(f\"{word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2b7f3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def extract_top_words(train_set_filtered, num_words=20):\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        train_set_filtered['filtered_text'],\n",
    "        train_set_filtered['result'],\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Create the TF-IDF vectorizer object\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.8, min_df=5)\n",
    "\n",
    "    # Fit and transform the vectorizer on the train set\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "    # Extract the top words with the highest scores for the train set\n",
    "    train_scores = np.asarray(X_train_tfidf.mean(axis=0)).ravel().tolist()\n",
    "    train_features = tfidf_vectorizer.get_feature_names_out()\n",
    "    top_train_indices = np.argsort(train_scores)[::-1][:num_words]\n",
    "    top_train_words = [train_features[i] for i in top_train_indices]\n",
    "    top_train_scores = [train_scores[i] for i in top_train_indices]\n",
    "\n",
    "    # Transform the vectorizer on the test set\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    # Extract the top words with the highest scores for the test set\n",
    "    test_scores = np.asarray(X_test_tfidf.mean(axis=0)).ravel().tolist()\n",
    "    test_features = tfidf_vectorizer.get_feature_names_out()\n",
    "    top_test_indices = np.argsort(test_scores)[::-1][:num_words]\n",
    "    top_test_words = [test_features[i] for i in top_test_indices]\n",
    "    top_test_scores = [test_scores[i] for i in top_test_indices]\n",
    "\n",
    "    # Return the top words and scores for both train and test sets\n",
    "    return {\n",
    "        'train': {\n",
    "            'words': top_train_words,\n",
    "            'scores': top_train_scores\n",
    "        },\n",
    "        'test': {\n",
    "            'words': top_test_words,\n",
    "            'scores': top_test_scores\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7955f75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 words with the highest scores for the train set:\n",
      "relatorio: 0.0733\n",
      "clinico: 0.0701\n",
      "epilepsia: 0.0313\n",
      "nao: 0.0304\n",
      "alteracoes: 0.0274\n",
      "ha: 0.0256\n",
      "neurologia: 0.0219\n",
      "demencia: 0.0207\n",
      "mg: 0.0190\n",
      "tac: 0.0181\n",
      "se: 0.0180\n",
      "tremor: 0.0175\n",
      "cefaleias: 0.0169\n",
      "quadro: 0.0168\n",
      "avaliacao: 0.0167\n",
      "ce: 0.0165\n",
      "hta: 0.0162\n",
      "memoria: 0.0155\n",
      "agravamento: 0.0152\n",
      "antecedentes: 0.0149\n",
      "\n",
      "Top 20 words with the highest scores for the test set:\n",
      "relatorio: 0.0893\n",
      "clinico: 0.0866\n",
      "nao: 0.0324\n",
      "tremor: 0.0302\n",
      "alteracoes: 0.0292\n",
      "epilepsia: 0.0269\n",
      "ha: 0.0269\n",
      "se: 0.0258\n",
      "memoria: 0.0240\n",
      "demencia: 0.0217\n",
      "tac: 0.0210\n",
      "marcha: 0.0197\n",
      "antecedentes: 0.0188\n",
      "mg: 0.0184\n",
      "ce: 0.0180\n",
      "informacao: 0.0169\n",
      "neurologia: 0.0168\n",
      "hta: 0.0157\n",
      "meses: 0.0157\n",
      "dra: 0.0153\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have the `train_set_filtered` DataFrame\n",
    "result = extract_top_words(train_set_filtered, num_words=20)\n",
    "\n",
    "print(\"Top 20 words with the highest scores for the train set:\")\n",
    "for word, score in zip(result['train']['words'], result['train']['scores']):\n",
    "    print(f\"{word}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 20 words with the highest scores for the test set:\")\n",
    "for word, score in zip(result['test']['words'], result['test']['scores']):\n",
    "    print(f\"{word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe141716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_words(train_set_filtered, num_words=20):\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        train_set_filtered['filtered_text'],\n",
    "        train_set_filtered['result'],\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Create the TF-IDF vectorizer object\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.8, min_df=5)\n",
    "\n",
    "    # Fit and transform the vectorizer on the train set\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "    # Extract the top words with the highest scores for the train set\n",
    "    train_scores = np.asarray(X_train_tfidf.mean(axis=0)).ravel().tolist()\n",
    "    train_features = tfidf_vectorizer.get_feature_names_out()\n",
    "    top_train_indices = np.argsort(train_scores)[::-1][:num_words]\n",
    "    top_train_words = [train_features[i] for i in top_train_indices]\n",
    "    top_train_scores = [train_scores[i] for i in top_train_indices]\n",
    "\n",
    "    # Transform the vectorizer on the test set\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    # Extract the top words with the highest scores for the test set\n",
    "    test_scores = np.asarray(X_test_tfidf.mean(axis=0)).ravel().tolist()\n",
    "    test_features = tfidf_vectorizer.get_feature_names_out()\n",
    "    top_test_indices = np.argsort(test_scores)[::-1][:num_words]\n",
    "    top_test_words = [test_features[i] for i in top_test_indices]\n",
    "    top_test_scores = [test_scores[i] for i in top_test_indices]\n",
    "\n",
    "    # Return the top words and scores for both train and test sets\n",
    "    return {\n",
    "        'train': {\n",
    "            'words': top_train_words,\n",
    "            'scores': top_train_scores\n",
    "        },\n",
    "        'test': {\n",
    "            'words': top_test_words,\n",
    "            'scores': top_test_scores\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Assuming you have the `train_set_filtered` DataFrame\n",
    "num_words = 20\n",
    "top_words = extract_top_words(train_set_filtered, num_words=num_words)\n",
    "\n",
    "# Create a new DataFrame to store the extracted features\n",
    "extracted_features = pd.DataFrame()\n",
    "\n",
    "# Iterate over each row in the original dataset\n",
    "for index, row in train_set_filtered.iterrows():\n",
    "    word_scores = []\n",
    "    # Compute the word scores for the top words\n",
    "    for word in top_words['train']['words']:\n",
    "        # Compute the word frequency or TF-IDF score for the current row\n",
    "        score = row['filtered_text'].count(word)  # Example: Using word frequency\n",
    "        word_scores.append(score)\n",
    "    # Append the word scores to the new DataFrame\n",
    "    extracted_features = extracted_features.append(pd.Series(word_scores), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbaf7363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1285 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  \\\n",
       "0      0   0   0   1   0   0   0   0   0   0   1   1   0   0   0   1   0   0   \n",
       "1      1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "2      2   0   1   2   0   0   1   0   0   0   3   0   0   0   0   1   0   0   \n",
       "3      0   0   0   1   0   1   0   0   0   0   2   1   0   0   0   0   0   0   \n",
       "4      0   0   0   0   0   2   0   0   0   0   1   0   0   0   0   0   0   0   \n",
       "...   ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..   \n",
       "1280   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   1   \n",
       "1281   0   0   0   0   0   1   3   4   1   1   4   0   0   1   2   2   0   3   \n",
       "1282   0   0   1   0   0   0   0   0   1   0   4   0   0   0   1   3   0   0   \n",
       "1283   0   0   0   3   3   3   0   0   0   0   9   0   0   1   0   6   1   1   \n",
       "1284   0   0   0   0   0   0   0   0   0   0   2   0   0   0   0   1   0   0   \n",
       "\n",
       "      18  19  \n",
       "0      1   0  \n",
       "1      0   0  \n",
       "2      0   0  \n",
       "3      0   0  \n",
       "4      0   0  \n",
       "...   ..  ..  \n",
       "1280   0   0  \n",
       "1281   3   0  \n",
       "1282   0   1  \n",
       "1283   0   0  \n",
       "1284   0   0  \n",
       "\n",
       "[1285 rows x 20 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d24a84d",
   "metadata": {},
   "source": [
    "The numbers in the example represent the word scores for the top words extracted from the text data. Each row corresponds to a document or instance in the dataset, and each column represents a specific word.\n",
    "\n",
    "This row indicates the word scores for the top words in the document. Each number represents the score of the corresponding word at the same index. In this case, the top words are represented by their indices from 0 to 19.\n",
    "\n",
    "For example, the score for word 0 is 0, the score for word 1 is 0, the score for word 2 is 0, and so on. The score for word 4 is 1, indicating that it appears once in the document.\n",
    "\n",
    "Similarly, the following rows represent the word scores for other documents in the dataset. Each row represents a different document, and the numbers in the row indicate the scores for the corresponding words in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a603856f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
