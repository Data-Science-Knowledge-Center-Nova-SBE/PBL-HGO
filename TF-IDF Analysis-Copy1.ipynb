{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "883aa1a1",
   "metadata": {},
   "source": [
    "# TF-IDF Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95d1c94",
   "metadata": {},
   "source": [
    "TF-IDF (term frequency-inverse document frequency) analysis is a statistical technique used in natural language processing and information retrieval to determine the importance of a word in a document or corpus. It is a way to measure how relevant a word is to a document in a collection of documents.\n",
    "\n",
    "TF-IDF analysis assigns a weight to each word in a document based on how frequently it appears in the document (term frequency) and how rare it is in the entire corpus (inverse document frequency). The weight assigned to a word increases proportionally with its frequency in the document, but is offset by the rarity of the word in the corpus. This means that words that appear frequently in a document but also appear frequently in many other documents in the corpus are given a lower weight, while words that appear less frequently in the corpus but frequently in a particular document are given a higher weight.\n",
    "\n",
    "The output of TF-IDF analysis is a numerical representation of each document that captures the importance of each word in that document. This can be used for various tasks such as text classification, clustering, and information retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ffe588",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Connect to Database ](#Connect-to-database)\n",
    "* [Import Datasets](#Import-Dataset)\n",
    "* [Remove Stopwords](#Remove-stopwords)\n",
    "* [Lemmatization](#Lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b973cc15",
   "metadata": {},
   "source": [
    "## Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee28e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "#creds = [\"username\",\"password\",\"juliehaegh\",\"ninG20&19rea\",\"3306\"] \n",
    "creds = [\"juliehaegh\",\"ninG20&19rea\",\"172.20.20.4\",\"hgo\",3306]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11097233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables: [('ConsultaUrgencia_doentespedidosconsultaNeurologia2012',), ('consultaneurologia2012',), ('consultaneurologia201216anon_true',), ('hgo_data_032023',)]\n",
      "312\n"
     ]
    }
   ],
   "source": [
    "#Connection to the database\n",
    "host = creds[2]\n",
    "user = creds[0]\n",
    "password = creds[1]\n",
    "database = creds[3]\n",
    "port = creds[4]\n",
    "mydb = mysql.connector.connect(host=host, user=user, database=database, port=port, password=password, auth_plugin='mysql_native_password')\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "#Safecheck to guarantee that the connection worked\n",
    "mycursor.execute('SHOW TABLES;')\n",
    "print(f\"Tables: {mycursor.fetchall()}\")\n",
    "print(mydb.connection_id) #it'll give connection_id,if got connected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0b9ef7",
   "metadata": {},
   "source": [
    "## Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a031a49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliehaegh/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/Users/juliehaegh/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import Alert P1 dataset\n",
    "SClinic = pd.read_sql(\"\"\"SELECT * FROM ConsultaUrgencia_doentespedidosconsultaNeurologia2012\"\"\",mydb)\n",
    "\n",
    "# Import SClinic\n",
    "AlertP1 = pd.read_sql(\"\"\"SELECT * FROM consultaneurologia201216anon_true\"\"\",mydb)\n",
    "\n",
    "# Replace all NaN with 0\n",
    "AlertP1 = AlertP1.fillna(0)\n",
    "\n",
    "# Add result column\n",
    "AlertP1['result'] = ['Accepted' if x in [0,14,25,20,53,8,12,12] else 'Refused' for x in AlertP1['COD_MOTIVO_RECUSA']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0215a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with accepted and rejected cases\n",
    "#SClinic['Accepted/Rejected'] = SClinic['COD_MOTIVO_RECUSA'].apply(lambda x: 'Accepted' if x == 0 else 'Rejected')\n",
    "#SClinic = SClinic[(SClinic['Texto']!='') & (SClinic['Accepted/Rejected']=='Accepted')].iloc[887:987]\n",
    "#SClinic = SClinic[SClinic['Texto']!='']\n",
    "#SClinic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca45849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Split data into train and test\n",
    "AlertP1_sorted = AlertP1[AlertP1['Texto']!=''].sort_values(by='DATA_RECEPCAO')\n",
    "\n",
    "# calculate the index for the split\n",
    "split_index = math.ceil(0.8 * len(AlertP1_sorted))\n",
    "\n",
    "# split the data frame into test and train sets\n",
    "train_set = AlertP1_sorted.iloc[:split_index]\n",
    "test_set = AlertP1_sorted.iloc[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2b7ea14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juliehaegh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/juliehaegh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import librariers \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a088fb",
   "metadata": {},
   "source": [
    "## Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9b4873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of special characters and transform Texto column to Latin words\n",
    "train_set['Texto'] = train_set['Texto'].apply(lambda x: unidecode(x))\n",
    "\n",
    "#The re.sub function is used to substitute all digits (\\d) with an empty string\n",
    "train_set['Texto'] = train_set['Texto'].apply(lambda x: re.sub(r'\\d', '', x))\n",
    "\n",
    "# Remove all names in Texto variable\n",
    "# This function uses a regular expression to find all words in the text that start with a \n",
    "# capital letter (\\b[A-Z][a-z]+\\b), which are assumed to be names\n",
    "text = train_set['Texto'] \n",
    "\n",
    "# remove all hyphens from the text\n",
    "text = text.replace('-', '')\n",
    "\n",
    "def remove_names(text):\n",
    "    # Find all words that start with a capital letter\n",
    "    names = re.findall(r'\\b[A-Z][a-z]+\\b', text)\n",
    "    \n",
    "    # Replace the names with an empty string\n",
    "    for name in names:\n",
    "        text = text.replace(name, '')\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2069e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the text\n",
    "text_list = []\n",
    "\n",
    "# Loop through the 'text' column\n",
    "for text in text.str.lower(): # Transform every word to lower case\n",
    "    text_list.append(text)\n",
    "\n",
    "# Print the list of text\n",
    "#print(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2e7d46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juliehaegh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/juliehaegh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the Portuguese stop words\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Get the Portuguese stop words\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Manually remove stopwords\n",
    "stop_words.update(['-//','.', ',','(',')',':','-','?','+','/',';','2','1','drª','``','','3','desde','anos','doente','consulta','alterações','se',\"''\",'cerca','refere','hgo','utente','vossa','s','...','ainda','c','filha','costa','dr.','pereira','ja','--','p','dr','h','n','>','q','//','..','b','++','%','//','-','+++/','=','+++/'])\n",
    "\n",
    "# Create a new list to store the filtered text\n",
    "filtered_text = []\n",
    "\n",
    "# Loop through the text_list and remove the stop words\n",
    "for text in text_list:\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    filtered_text.append(\" \".join(words))\n",
    "\n",
    "# Print the filtered text\n",
    "#print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b63efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the filtered text as a new column to the dataframe\n",
    "train_set['filtered_text'] = filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95691b55",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e87d5",
   "metadata": {},
   "source": [
    "Lemmatization is a text normalization technique used in Natural Language Processing (NLP), that switches any kind of a word to its base root mode. Lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "449168bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for lemmatization\n",
    "def spacy_lemmatizer(df):\n",
    "    import spacy\n",
    "    import pt_core_news_md\n",
    "    nlp = pt_core_news_md.load()\n",
    "\n",
    "    doclist = list(nlp.pipe(df))\n",
    "\n",
    "    docs=[]\n",
    "    for i, doc in enumerate(doclist):\n",
    "        docs.append(' '.join([listitem.lemma_ for listitem in doc]))\n",
    "        \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c39bfb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list to store the words\n",
    "word_list = []\n",
    "\n",
    "# loop through each row of the \"text_column\" column\n",
    "for index, row in train_set.iterrows():\n",
    "    \n",
    "    # split the text into individual words using whitespace as a delimiter\n",
    "    words = row['filtered_text'].split()\n",
    "    # add the words to the word list\n",
    "    word_list.extend(words)\n",
    "\n",
    "# print the word list\n",
    "#print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2657afaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list to store the words\n",
    "word_list = []\n",
    "\n",
    "# loop through each row of the \"text_column\" column\n",
    "for index, row in train_set.iterrows():\n",
    "    \n",
    "    # split the text into individual words using whitespace as a delimiter\n",
    "    words = row['filtered_text'].split()\n",
    "    \n",
    "    # remove hyphens from the words and add them to the word list\n",
    "    word_list.extend([word.replace('-', '') for word in words])\n",
    "    # remove slash from the words and ass them to the list\n",
    "    word_list.extend([word.replace('/', '') for word in words])\n",
    "    \n",
    "\n",
    "# print the cleaned word list\n",
    "#print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56096fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words after lemmatization: 8793\n",
      "The number of words before lemmatization: 10712\n"
     ]
    }
   ],
   "source": [
    "Lemma = spacy_lemmatizer(word_list) # Call lemmatizer function\n",
    "\n",
    "# print length of word_list and compare the count after doing lemmatization\n",
    "from collections import Counter\n",
    "\n",
    "items = Counter(Lemma).keys()\n",
    "print('The number of words after lemmatization:',len(items))\n",
    "\n",
    "items2 = Counter(word_list).keys()\n",
    "print('The number of words before lemmatization:',len(items2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98955bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>filtered_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>dor lapso efoi-le dar alto qualquer justificac...</td>\n",
       "      <td>dor lapso foi-lhe dada alta qualquer justifica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>relatorio clinico</td>\n",
       "      <td>relatorio clinico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>homem ap dm gamapatia monoclonal igm dca arter...</td>\n",
       "      <td>homem ap dm gamapatia monoclonal igm dca arter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>mulher dor ponto lingua sensacao repuxamento l...</td>\n",
       "      <td>mulher dor ponta lingua sensacao repuxamento l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>epilepsia</td>\n",
       "      <td>epilepsia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>justificacao optimizacao diagnosticar terapeut...</td>\n",
       "      <td>justificacao optimizacao diagnostica terapeuti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>referencia duplicar</td>\n",
       "      <td>referencia duplicada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>referencia duplicar</td>\n",
       "      <td>referencia duplicada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>historia actual problema saude resolver parkin...</td>\n",
       "      <td>historia actual problema saude resolver parkin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>relatorio clinico</td>\n",
       "      <td>relatorio clinico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1416 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text_lemmatized  \\\n",
       "1540  dor lapso efoi-le dar alto qualquer justificac...   \n",
       "525                                   relatorio clinico   \n",
       "121   homem ap dm gamapatia monoclonal igm dca arter...   \n",
       "168   mulher dor ponto lingua sensacao repuxamento l...   \n",
       "1154                                          epilepsia   \n",
       "...                                                 ...   \n",
       "1619  justificacao optimizacao diagnosticar terapeut...   \n",
       "902                                 referencia duplicar   \n",
       "901                                 referencia duplicar   \n",
       "1105  historia actual problema saude resolver parkin...   \n",
       "624                                   relatorio clinico   \n",
       "\n",
       "                                          filtered_text  \n",
       "1540  dor lapso foi-lhe dada alta qualquer justifica...  \n",
       "525                                   relatorio clinico  \n",
       "121   homem ap dm gamapatia monoclonal igm dca arter...  \n",
       "168   mulher dor ponta lingua sensacao repuxamento l...  \n",
       "1154                                          epilepsia  \n",
       "...                                                 ...  \n",
       "1619  justificacao optimizacao diagnostica terapeuti...  \n",
       "902                                referencia duplicada  \n",
       "901                                referencia duplicada  \n",
       "1105  historia actual problema saude resolver parkin...  \n",
       "624                                   relatorio clinico  \n",
       "\n",
       "[1416 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the spacy_lemmatizer function to each row in the 'text' column\n",
    "train_set['text_lemmatized'] = spacy_lemmatizer(train_set['filtered_text'])\n",
    "\n",
    "# drop rows with empty strings\n",
    "train_set_filtered = train_set[['text_lemmatized','filtered_text']].replace('', pd.NA).dropna()\n",
    "train_set_filtered = pd.DataFrame(train_set_filtered)\n",
    "train_set_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db720ac",
   "metadata": {},
   "source": [
    "## Calculate distance between words using 'Jarowynkler'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12c2726",
   "metadata": {},
   "source": [
    "In natural language processing and text mining, the distance between words refers to the measure of how dissimilar or different two words are in terms of their spelling, meaning, or context. It is used to compare two words or to quantify the similarity between them.\n",
    "\n",
    "These distance metrics are useful in many natural language processing tasks, such as spell checking, text classification, clustering, and information retrieval, among others. They enable us to quantify the similarity or dissimilarity between words or texts and to make data-driven decisions based on this information.\n",
    "\n",
    "In the code below, we will just measure the distance between medication words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28684a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import medication list\n",
    "drug_name = pd.read_excel('/Users/juliehaegh/Downloads/drugs_data.xlsx')\n",
    "\n",
    "# Convert string to list\n",
    "drug_list = drug_name.drug_name.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a445150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "# create an empty list to store the distances\n",
    "distances = []\n",
    "\n",
    "# loop through each pair of adjacent words in the word list\n",
    "for i in range(len(word_list)-1):\n",
    "    \n",
    "    # calculate the Levenshtein distance between the current word and the next word\n",
    "    distance = Levenshtein.distance(word_list[i], word_list[i+1])\n",
    "    \n",
    "    # add the distance to the list of distances\n",
    "    distances.append(distance)\n",
    "\n",
    "# print the list of distances\n",
    "#print(len(distances))\n",
    "#print(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d24a1c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "# create an empty list to store the distances\n",
    "distances = []\n",
    "\n",
    "# loop through each pair of adjacent words in the word list\n",
    "for i in range(len(word_list)-1):\n",
    "    \n",
    "    # calculate the Levenshtein distance between the current word and the next word\n",
    "    distance = Levenshtein.distance(word_list[i], word_list[i+1])\n",
    "    \n",
    "    # add the distance to the list of distances\n",
    "    distances.append(distance)\n",
    "\n",
    "# merge adjacent words with a distance lower than 1\n",
    "merged_word_list = []\n",
    "i = 0\n",
    "while i < len(word_list):\n",
    "    if i == len(word_list) - 1:\n",
    "        # last word in the list, add it to the merged word list\n",
    "        merged_word_list.append(word_list[i])\n",
    "        i += 1\n",
    "    elif distances[i] < 1:\n",
    "        # merge the current and next word into a single word and add it to the merged word list\n",
    "        merged_word_list.append(word_list[i] + word_list[i+1])\n",
    "        i += 2\n",
    "    else:\n",
    "        # add the current word to the merged word list\n",
    "        merged_word_list.append(word_list[i])\n",
    "        i += 1\n",
    "\n",
    "# print the merged word list\n",
    "#print(merged_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e843210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# create a CountVectorizer object\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# fit and transform the text data\n",
    "count_matrix = count_vectorizer.fit_transform(train_set_filtered['text_lemmatized'])\n",
    "\n",
    "# create a TfidfVectorizer object\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# fit and transform the text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_set_filtered['text_lemmatized'])\n",
    "\n",
    "# print the document-term matrix for CountVectorizer\n",
    "#print(\"Count Vectorizer:\\n\")\n",
    "#print(pd.DataFrame(count_matrix.toarray(), columns=count_vectorizer.get_feature_names_out()))\n",
    "\n",
    "# print the document-term matrix for TfidfVectorizer\n",
    "#print(\"\\nTF-IDF Vectorizer:\\n\")\n",
    "#print(pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb6fc315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer:\n",
      "\n",
      "\n",
      "TF-IDF Vectorizer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a CountVectorizer object\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# fit and transform the text data\n",
    "#count_matrix = count_vectorizer.fit_transform(train_set_filtered['text_lemmatized'])\n",
    "count_matrix = count_vectorizer.fit_transform(train_set['text_lemmatized'])\n",
    "\n",
    "# create a dataframe for CountVectorizer output\n",
    "count_df = pd.DataFrame(count_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# create a TfidfVectorizer object\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# fit and transform the text data\n",
    "#tfidf_matrix = tfidf_vectorizer.fit_transform(train_set_filtered['text_lemmatized'])\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_set['text_lemmatized'])\n",
    "\n",
    "# create a dataframe for TfidfVectorizer output\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# print the document-term matrix for CountVectorizer\n",
    "print(\"Count Vectorizer:\\n\")\n",
    "count_df\n",
    "\n",
    "# print the document-term matrix for TfidfVectorizer\n",
    "print(\"\\nTF-IDF Vectorizer:\\n\")\n",
    "tfidf_df\n",
    "tfidf_df.to_csv('tf-idf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff30c927",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = tfidf_df.max().sort_values(ascending=False).head(100)\n",
    "#tf_idf.to_csv('tf-idf.csv')\n",
    "#mean_scores = tfidf_df.mean()\n",
    "#mean_scores.sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4811cda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>2e</th>\n",
       "      <th>aa</th>\n",
       "      <th>aacentuar</th>\n",
       "      <th>aas</th>\n",
       "      <th>aat</th>\n",
       "      <th>ab</th>\n",
       "      <th>abaixo</th>\n",
       "      <th>abandonar</th>\n",
       "      <th>abandono</th>\n",
       "      <th>...</th>\n",
       "      <th>zonegr</th>\n",
       "      <th>zonegram</th>\n",
       "      <th>zonegran</th>\n",
       "      <th>zonisamer</th>\n",
       "      <th>zoster</th>\n",
       "      <th>zotepino</th>\n",
       "      <th>zumbido</th>\n",
       "      <th>zyloric</th>\n",
       "      <th>zyprexa</th>\n",
       "      <th>zyprexo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>1414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>1415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>1416</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>1417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>1418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1419 rows × 8188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0   2e   aa  aacentuar  aas  aat   ab  abaixo  abandonar  \\\n",
       "0              0  0.0  0.0        0.0  0.0  0.0  0.0     0.0        0.0   \n",
       "1              1  0.0  0.0        0.0  0.0  0.0  0.0     0.0        0.0   \n",
       "2              2  0.0  0.0        0.0  0.0  0.0  0.0     0.0        0.0   \n",
       "3              3  0.0  0.0        0.0  0.0  0.0  0.0     0.0        0.0   \n",
       "4              4  0.0  0.0        0.0  0.0  0.0  0.0     0.0        0.0   \n",
       "...          ...  ...  ...        ...  ...  ...  ...     ...        ...   \n",
       "1414        1414  0.0  0.0        0.0  0.0  0.0  0.0     0.0        0.0   \n",
       "1415        1415  0.0  0.0        0.0  0.0  0.0  0.0     0.0        0.0   \n",
       "1416        1416  0.0  0.0        0.0  0.0  0.0  0.0     0.0        0.0   \n",
       "1417        1417  0.0  0.0        0.0  0.0  0.0  0.0     0.0        0.0   \n",
       "1418        1418  0.0  0.0        0.0  0.0  0.0  0.0     0.0        0.0   \n",
       "\n",
       "      abandono  ...  zonegr  zonegram  zonegran  zonisamer  zoster  zotepino  \\\n",
       "0          0.0  ...     0.0       0.0       0.0        0.0     0.0       0.0   \n",
       "1          0.0  ...     0.0       0.0       0.0        0.0     0.0       0.0   \n",
       "2          0.0  ...     0.0       0.0       0.0        0.0     0.0       0.0   \n",
       "3          0.0  ...     0.0       0.0       0.0        0.0     0.0       0.0   \n",
       "4          0.0  ...     0.0       0.0       0.0        0.0     0.0       0.0   \n",
       "...        ...  ...     ...       ...       ...        ...     ...       ...   \n",
       "1414       0.0  ...     0.0       0.0       0.0        0.0     0.0       0.0   \n",
       "1415       0.0  ...     0.0       0.0       0.0        0.0     0.0       0.0   \n",
       "1416       0.0  ...     0.0       0.0       0.0        0.0     0.0       0.0   \n",
       "1417       0.0  ...     0.0       0.0       0.0        0.0     0.0       0.0   \n",
       "1418       0.0  ...     0.0       0.0       0.0        0.0     0.0       0.0   \n",
       "\n",
       "      zumbido  zyloric  zyprexa  zyprexo  \n",
       "0         0.0      0.0      0.0      0.0  \n",
       "1         0.0      0.0      0.0      0.0  \n",
       "2         0.0      0.0      0.0      0.0  \n",
       "3         0.0      0.0      0.0      0.0  \n",
       "4         0.0      0.0      0.0      0.0  \n",
       "...       ...      ...      ...      ...  \n",
       "1414      0.0      0.0      0.0      0.0  \n",
       "1415      0.0      0.0      0.0      0.0  \n",
       "1416      0.0      0.0      0.0      0.0  \n",
       "1417      0.0      0.0      0.0      0.0  \n",
       "1418      0.0      0.0      0.0      0.0  \n",
       "\n",
       "[1419 rows x 8188 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tf-idf scores\n",
    "tfidf_data = pd.read_csv('tf-idf.csv')\n",
    "tfidf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47189eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all NaN with 0\n",
    "tfidf_data = tfidf_data.fillna(0)\n",
    "#tfidf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21788d86",
   "metadata": {},
   "source": [
    "Next, concatenate the SClinic dataset with the tfidf_data dataframe using the concat() function from the pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df4b2eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID_DOENTE              296\n",
      "PROCESSO               296\n",
      "COD_REFERENCIA         296\n",
      "COD_PZ                 296\n",
      "COD_UNID_SAUDE_PROV    296\n",
      "                      ... \n",
      "zotepino               296\n",
      "zumbido                296\n",
      "zyloric                296\n",
      "zyprexa                296\n",
      "zyprexo                296\n",
      "Length: 8222, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "AlertP1_tfidf = pd.concat([train_set, tfidf_data], axis=1)\n",
    "\n",
    "# Check for NaN values in a DataFrame called my_data\n",
    "print(AlertP1_tfidf.isnull().sum())\n",
    "\n",
    "# Remove NaN values from my_data\n",
    "AlertP1_tfidf.dropna(inplace=True)\n",
    "\n",
    "# Replace NaN values with 0 in my_data\n",
    "AlertP1_tfidf.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e2049b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def log_regression(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    log_reg = LogisticRegression()\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    y_pred = log_reg.predict(X_test)\n",
    "    coefficients = log_reg.coef_\n",
    "    intercept = log_reg.intercept_\n",
    "    return y_pred, coefficients, intercept, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "196e24c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5810</th>\n",
       "      <td>parkinson</td>\n",
       "      <td>0.935249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6092</th>\n",
       "      <td>polineuropatia</td>\n",
       "      <td>0.921760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3235</th>\n",
       "      <td>falecer</td>\n",
       "      <td>0.844074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3926</th>\n",
       "      <td>historia</td>\n",
       "      <td>0.753930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6731</th>\n",
       "      <td>remarcacao</td>\n",
       "      <td>0.747490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4886</th>\n",
       "      <td>medicacao</td>\n",
       "      <td>0.692385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6652</th>\n",
       "      <td>referencia</td>\n",
       "      <td>0.651676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4933</th>\n",
       "      <td>memoria</td>\n",
       "      <td>0.634805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>dca</td>\n",
       "      <td>0.631306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>cronico</td>\n",
       "      <td>0.625498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270</th>\n",
       "      <td>fazer</td>\n",
       "      <td>0.621664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>bradicinesio</td>\n",
       "      <td>0.610917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>convulsivo</td>\n",
       "      <td>0.605301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5450</th>\n",
       "      <td>novo</td>\n",
       "      <td>0.596869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>controlar</td>\n",
       "      <td>0.575638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>cefaleia</td>\n",
       "      <td>0.563531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5922</th>\n",
       "      <td>perder</td>\n",
       "      <td>0.548054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3660</th>\n",
       "      <td>grande</td>\n",
       "      <td>0.539767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>ataxio</td>\n",
       "      <td>0.534604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7040</th>\n",
       "      <td>seguir</td>\n",
       "      <td>0.527021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Feature  coefficients\n",
       "5810       parkinson      0.935249\n",
       "6092  polineuropatia      0.921760\n",
       "3235         falecer      0.844074\n",
       "3926        historia      0.753930\n",
       "6731      remarcacao      0.747490\n",
       "4886       medicacao      0.692385\n",
       "6652      referencia      0.651676\n",
       "4933         memoria      0.634805\n",
       "1993             dca      0.631306\n",
       "1935         cronico      0.625498\n",
       "3270           fazer      0.621664\n",
       "1073    bradicinesio      0.610917\n",
       "1803      convulsivo      0.605301\n",
       "5450            novo      0.596869\n",
       "1774       controlar      0.575638\n",
       "1310        cefaleia      0.563531\n",
       "5922          perder      0.548054\n",
       "3660          grande      0.539767\n",
       "789           ataxio      0.534604\n",
       "7040          seguir      0.527021"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "eliminate_cols=['ID_DOENTE','PROCESSO','COD_REFERENCIA','result','COD_PZ','COD_UNID_SAUDE_PROV','UNID_PROV','TIPO_UNID','COD_CTH_PRIOR','CTH_PRIOR','text_lemmatized','COD_MOTIVO_RECUSA','DES_MOTIVO_RECUSA','COD_ESPECIALIDADE','DES_ESPECIALIDADE','agrupadora','OUTRA_ENTIDADE','DATA_RECEPCAO','DATA_ENVIO','DATA_RETORNO','NUM_TAXA','ESTADO','DATA_MARCACAO','DATA_REALIZACAO','OBSERVACOES','Mês_entrada','Ano_entrada','trata data recusa','resume saída','mês_saida','ano_saida','filtered_text','Texto','PROVENIENCIA']\n",
    "X = AlertP1_tfidf.drop(eliminate_cols,axis=1)# Features\n",
    "y = AlertP1_tfidf['result']\n",
    "\n",
    "# Instantiate the LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the target variable and transform it\n",
    "y = le.fit_transform(y)\n",
    "features=X.columns\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X, y)\n",
    "\n",
    "#log_regression is defined in functions\n",
    "y_pred,coefficients,intercept,X_train, X_test, y_train, y_test=log_regression(X,y)\n",
    "df = pd.DataFrame(features, columns =['Feature'])\n",
    "df['coefficients']=coefficients.T\n",
    "df.sort_values(by= 'coefficients',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b63bfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6533333333333333\n",
      "Recall: 0.038461538461538464\n",
      "Precision: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     refused       0.66      0.98      0.79       147\n",
      "    accepted       0.50      0.04      0.07        78\n",
      "\n",
      "    accuracy                           0.65       225\n",
      "   macro avg       0.58      0.51      0.43       225\n",
      "weighted avg       0.60      0.65      0.54       225\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Evaluate the recall of the model\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Evaluate the precision of the model\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate the confusion matrix of the model\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "# Print the evaluation matrix\n",
    "target_names = ['refused', 'accepted']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227fc248",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
