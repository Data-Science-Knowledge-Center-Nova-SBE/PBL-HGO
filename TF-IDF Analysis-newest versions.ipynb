{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "883aa1a1",
   "metadata": {},
   "source": [
    "# TF-IDF Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95d1c94",
   "metadata": {},
   "source": [
    "TF-IDF (term frequency-inverse document frequency) analysis is a statistical technique used in natural language processing and information retrieval to determine the importance of a word in a document or corpus. It is a way to measure how relevant a word is to a document in a collection of documents.\n",
    "\n",
    "TF-IDF analysis assigns a weight to each word in a document based on how frequently it appears in the document (term frequency) and how rare it is in the entire corpus (inverse document frequency). The weight assigned to a word increases proportionally with its frequency in the document, but is offset by the rarity of the word in the corpus. This means that words that appear frequently in a document but also appear frequently in many other documents in the corpus are given a lower weight, while words that appear less frequently in the corpus but frequently in a particular document are given a higher weight.\n",
    "\n",
    "The output of TF-IDF analysis is a numerical representation of each document that captures the importance of each word in that document. This can be used for various tasks such as text classification, clustering, and information retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ffe588",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Connect to Database ](#Connect-to-database)\n",
    "* [Import Datasets](#Import-Dataset)\n",
    "* [Remove Stopwords](#Remove-stopwords)\n",
    "* [Lemmatization](#Lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b973cc15",
   "metadata": {},
   "source": [
    "## Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee28e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "#creds = [\"username\",\"password\",\"juliehaegh\",\"ninG20&19rea\",\"3306\"] \n",
    "creds = [\"juliehaegh\",\"ninG20&19rea\",\"172.20.20.4\",\"hgo\",3306]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11097233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables: [('ConsultaUrgencia_doentespedidosconsultaNeurologia2012',), ('consultaneurologia2012',), ('consultaneurologia201216anon_true',), ('hgo_data_032023',)]\n",
      "420\n"
     ]
    }
   ],
   "source": [
    "#Connection to the database\n",
    "host = creds[2]\n",
    "user = creds[0]\n",
    "password = creds[1]\n",
    "database = creds[3]\n",
    "port = creds[4]\n",
    "mydb = mysql.connector.connect(host=host, user=user, database=database, port=port, password=password, auth_plugin='mysql_native_password')\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "#Safecheck to guarantee that the connection worked\n",
    "mycursor.execute('SHOW TABLES;')\n",
    "print(f\"Tables: {mycursor.fetchall()}\")\n",
    "print(mydb.connection_id) #it'll give connection_id,if got connected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0b9ef7",
   "metadata": {},
   "source": [
    "## Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a031a49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliehaegh/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/Users/juliehaegh/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import Alert P1 dataset\n",
    "SClinic = pd.read_sql(\"\"\"SELECT * FROM ConsultaUrgencia_doentespedidosconsultaNeurologia2012\"\"\",mydb)\n",
    "\n",
    "# Import SClinic\n",
    "AlertP1 = pd.read_sql(\"\"\"SELECT * FROM consultaneurologia201216anon_true\"\"\",mydb)\n",
    "\n",
    "# Replace all NaN with 0\n",
    "AlertP1 = AlertP1.fillna(0)\n",
    "\n",
    "# Add result column\n",
    "AlertP1['result'] = ['Accepted' if x in [0,14,25,20,53,8,12,12] else 'Refused' for x in AlertP1['COD_MOTIVO_RECUSA']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0215a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with accepted and rejected cases\n",
    "#SClinic['Accepted/Rejected'] = SClinic['COD_MOTIVO_RECUSA'].apply(lambda x: 'Accepted' if x == 0 else 'Rejected')\n",
    "#SClinic = SClinic[(SClinic['Texto']!='') & (SClinic['Accepted/Rejected']=='Accepted')].iloc[887:987]\n",
    "#SClinic = SClinic[SClinic['Texto']!='']\n",
    "#SClinic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca45849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Split data into train and test\n",
    "AlertP1_sorted = AlertP1[AlertP1['Texto']!=''].sort_values(by='DATA_RECEPCAO')\n",
    "\n",
    "# calculate the index for the split\n",
    "split_index = math.ceil(0.8 * len(AlertP1_sorted))\n",
    "\n",
    "# split the data frame into test and train sets\n",
    "train_set = AlertP1_sorted.iloc[:split_index]\n",
    "test_set = AlertP1_sorted.iloc[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2b7ea14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juliehaegh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/juliehaegh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import librariers \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a088fb",
   "metadata": {},
   "source": [
    "## Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9b4873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of special characters and transform Texto column to Latin words\n",
    "train_set['Texto'] = train_set['Texto'].apply(lambda x: unidecode(x))\n",
    "\n",
    "#The re.sub function is used to substitute all digits (\\d) with an empty string\n",
    "train_set['Texto'] = train_set['Texto'].apply(lambda x: re.sub(r'\\d', '', x))\n",
    "\n",
    "# Remove all names in Texto variable\n",
    "# This function uses a regular expression to find all words in the text that start with a \n",
    "# capital letter (\\b[A-Z][a-z]+\\b), which are assumed to be names\n",
    "text = train_set['Texto'] \n",
    "\n",
    "# remove all hyphens from the text\n",
    "text = text.replace('-', '')\n",
    "\n",
    "def remove_names(text):\n",
    "    # Find all words that start with a capital letter\n",
    "    names = re.findall(r'\\b[A-Z][a-z]+\\b', text)\n",
    "    \n",
    "    # Replace the names with an empty string\n",
    "    for name in names:\n",
    "        text = text.replace(name, '')\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2069e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the text\n",
    "text_list = []\n",
    "\n",
    "# Loop through the 'text' column\n",
    "for text in text.str.lower(): # Transform every word to lower case\n",
    "    text_list.append(text)\n",
    "\n",
    "# Print the list of text\n",
    "#print(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2e7d46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juliehaegh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/juliehaegh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the Portuguese stop words\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Get the Portuguese stop words\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Manually remove stopwords\n",
    "stop_words.update(['-//','.', ',','(',')',':','-','?','+','/',';','2','1','drª','``','','3','desde','anos','doente','consulta','alterações','se',\"''\",'cerca','refere','hgo','utente','vossa','s','...','ainda','c','filha','costa','dr.','pereira','ja','--','p','dr','h','n','>','q','//','..','b','++','%','//','-','+++/','=','+++/'])\n",
    "\n",
    "# Create a new list to store the filtered text\n",
    "filtered_text = []\n",
    "\n",
    "# Loop through the text_list and remove the stop words\n",
    "for text in text_list:\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    filtered_text.append(\" \".join(words))\n",
    "\n",
    "# Print the filtered text\n",
    "#print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b63efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the filtered text as a new column to the dataframe\n",
    "train_set['filtered_text'] = filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95691b55",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e87d5",
   "metadata": {},
   "source": [
    "Lemmatization is a text normalization technique used in Natural Language Processing (NLP), that switches any kind of a word to its base root mode. Lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "449168bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for lemmatization\n",
    "def spacy_lemmatizer(df):\n",
    "    import spacy\n",
    "    import pt_core_news_md\n",
    "    nlp = pt_core_news_md.load()\n",
    "\n",
    "    doclist = list(nlp.pipe(df))\n",
    "\n",
    "    docs=[]\n",
    "    for i, doc in enumerate(doclist):\n",
    "        docs.append(' '.join([listitem.lemma_ for listitem in doc]))\n",
    "        \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c39bfb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list to store the words\n",
    "word_list = []\n",
    "\n",
    "# loop through each row of the \"text_column\" column\n",
    "for index, row in train_set.iterrows():\n",
    "    \n",
    "    # split the text into individual words using whitespace as a delimiter\n",
    "    words = row['filtered_text'].split()\n",
    "    # add the words to the word list\n",
    "    word_list.extend(words)\n",
    "\n",
    "# print the word list\n",
    "#print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2657afaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list to store the words\n",
    "word_list = []\n",
    "\n",
    "# loop through each row of the \"text_column\" column\n",
    "for index, row in train_set.iterrows():\n",
    "    \n",
    "    # split the text into individual words using whitespace as a delimiter\n",
    "    words = row['filtered_text'].split()\n",
    "    \n",
    "    # remove hyphens from the words and add them to the word list\n",
    "    word_list.extend([word.replace('-', '') for word in words])\n",
    "    # remove slash from the words and ass them to the list\n",
    "    word_list.extend([word.replace('/', '') for word in words])\n",
    "    \n",
    "\n",
    "# print the cleaned word list\n",
    "#print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56096fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words after lemmatization: 8793\n",
      "The number of words before lemmatization: 10712\n"
     ]
    }
   ],
   "source": [
    "Lemma = spacy_lemmatizer(word_list) # Call lemmatizer function\n",
    "\n",
    "# print length of word_list and compare the count after doing lemmatization\n",
    "from collections import Counter\n",
    "\n",
    "items = Counter(Lemma).keys()\n",
    "print('The number of words after lemmatization:',len(items))\n",
    "\n",
    "items2 = Counter(word_list).keys()\n",
    "print('The number of words before lemmatization:',len(items2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98955bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>filtered_text</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>dor lapso efoi-le dar alto qualquer justificac...</td>\n",
       "      <td>dor lapso foi-lhe dada alta qualquer justifica...</td>\n",
       "      <td>Refused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>relatorio clinico</td>\n",
       "      <td>relatorio clinico</td>\n",
       "      <td>Refused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>homem ap dm gamapatia monoclonal igm dca arter...</td>\n",
       "      <td>homem ap dm gamapatia monoclonal igm dca arter...</td>\n",
       "      <td>Refused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>mulher dor ponto lingua sensacao repuxamento l...</td>\n",
       "      <td>mulher dor ponta lingua sensacao repuxamento l...</td>\n",
       "      <td>Refused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>epilepsia</td>\n",
       "      <td>epilepsia</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>justificacao optimizacao diagnosticar terapeut...</td>\n",
       "      <td>justificacao optimizacao diagnostica terapeuti...</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>referencia duplicar</td>\n",
       "      <td>referencia duplicada</td>\n",
       "      <td>Refused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>referencia duplicar</td>\n",
       "      <td>referencia duplicada</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>historia actual problema saude resolver parkin...</td>\n",
       "      <td>historia actual problema saude resolver parkin...</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>relatorio clinico</td>\n",
       "      <td>relatorio clinico</td>\n",
       "      <td>Refused</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1416 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text_lemmatized  \\\n",
       "1540  dor lapso efoi-le dar alto qualquer justificac...   \n",
       "525                                   relatorio clinico   \n",
       "121   homem ap dm gamapatia monoclonal igm dca arter...   \n",
       "168   mulher dor ponto lingua sensacao repuxamento l...   \n",
       "1154                                          epilepsia   \n",
       "...                                                 ...   \n",
       "1619  justificacao optimizacao diagnosticar terapeut...   \n",
       "902                                 referencia duplicar   \n",
       "901                                 referencia duplicar   \n",
       "1105  historia actual problema saude resolver parkin...   \n",
       "624                                   relatorio clinico   \n",
       "\n",
       "                                          filtered_text    result  \n",
       "1540  dor lapso foi-lhe dada alta qualquer justifica...   Refused  \n",
       "525                                   relatorio clinico   Refused  \n",
       "121   homem ap dm gamapatia monoclonal igm dca arter...   Refused  \n",
       "168   mulher dor ponta lingua sensacao repuxamento l...   Refused  \n",
       "1154                                          epilepsia  Accepted  \n",
       "...                                                 ...       ...  \n",
       "1619  justificacao optimizacao diagnostica terapeuti...  Accepted  \n",
       "902                                referencia duplicada   Refused  \n",
       "901                                referencia duplicada  Accepted  \n",
       "1105  historia actual problema saude resolver parkin...  Accepted  \n",
       "624                                   relatorio clinico   Refused  \n",
       "\n",
       "[1416 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the spacy_lemmatizer function to each row in the 'text' column\n",
    "train_set['text_lemmatized'] = spacy_lemmatizer(train_set['filtered_text'])\n",
    "\n",
    "# drop rows with empty strings\n",
    "train_set_filtered = train_set[['text_lemmatized','filtered_text','result']].replace('', pd.NA).dropna()\n",
    "train_set_filtered = pd.DataFrame(train_set_filtered)\n",
    "train_set_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60f86469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lambda function to apply to the DataFrame\n",
    "train_set_filtered['accepted/rejected'] = train_set_filtered['result'].apply(lambda x: 1 if x == 'Accepted' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58cace48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into train and test sets with 20% test size\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_set_filtered['filtered_text'], train_set_filtered['accepted/rejected'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer object with max_df = 0.8 and min_df = 5\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, min_df=5)\n",
    "\n",
    "# Fit and transform the training data into a sparse matrix of TF-IDF features\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data into a sparse matrix of TF-IDF features\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Use the TF scores for future predictions\n",
    "tf_vectorizer = TfidfVectorizer(use_idf=False, max_df=0.8, min_df=5)\n",
    "X_tf = tf_vectorizer.fit_transform(train_set_filtered['filtered_text'])\n",
    "\n",
    "# Use the TF-IDF scores for logistic regression with other variables\n",
    "X_train_tfidf_lr = pd.concat([pd.DataFrame(X_train_tfidf.toarray()), train_set_filtered[train_set_filtered.index.isin(X_train.index)].drop(['filtered_text', 'accepted/rejected'], axis=1)], axis=1)\n",
    "X_test_tfidf_lr = pd.concat([pd.DataFrame(X_test_tfidf.toarray()), train_set_filtered[train_set_filtered.index.isin(X_test.index)].drop(['filtered_text', 'accepted/rejected'], axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbe2c3e",
   "metadata": {},
   "source": [
    "max_df is a float in the range [0.0, 1.0] or an integer, representing the maximum frequency of a term allowed in the corpus. Terms with a frequency higher than this threshold will be ignored as they are considered too common to be informative. In the code provided, we set max_df=0.8, which means that we are ignoring terms that appear in more than 80% of the documents in the corpus.\n",
    "\n",
    "min_df is also a float in the range [0.0, 1.0] or an integer, representing the minimum frequency of a term allowed in the corpus. Terms with a frequency lower than this threshold will be ignored as they are considered too rare to be informative. In the code provided, we set min_df=5, which means that we are ignoring terms that appear in less than 5 documents in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52e67ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1738"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e189f75",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Get the TF-IDF scores of the features for the train set\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_tfidf \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mtransform(\u001b[43mX_train_lr\u001b[49m)\n\u001b[1;32m      6\u001b[0m train_tfidf_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(train_tfidf\u001b[38;5;241m.\u001b[39mtoarray(), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Get the TF-IDF scores of the features for the test set\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_lr' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the names of the features\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Get the TF-IDF scores of the features for the train set\n",
    "train_tfidf = tfidf_vectorizer.transform(X_train_lr)\n",
    "train_tfidf_scores = np.mean(train_tfidf.toarray(), axis=0)\n",
    "\n",
    "# Get the TF-IDF scores of the features for the test set\n",
    "test_tfidf = tfidf_vectorizer.transform(X_test_lr)\n",
    "test_tfidf_scores = np.mean(test_tfidf.toarray(), axis=0)\n",
    "\n",
    "# Get the top 10 words with the highest scores for the train set\n",
    "top_train_indices = np.argsort(train_tfidf_scores)[::-1][:10]\n",
    "top_train_words = [feature_names[i] for i in top_train_indices]\n",
    "top_train_scores = [train_tfidf_scores[i] for i in top_train_indices]\n",
    "\n",
    "# Get the top 10 words with the highest scores for the test set\n",
    "top_test_indices = np.argsort(test_tfidf_scores)[::-1][:10]\n",
    "top_test_words = [feature_names[i] for i in top_test_indices]\n",
    "top_test_scores = [test_tfidf_scores[i] for i in top_test_indices]\n",
    "\n",
    "print(\"Top 10 words with the highest scores for the train set:\")\n",
    "for word, score in zip(top_train_words, top_train_scores):\n",
    "    print(f\"{word}: {score:.4f}\")\n",
    "    \n",
    "print(\"\\nTop 10 words with the highest scores for the test set:\")\n",
    "for word, score in zip(top_test_words, top_test_scores):\n",
    "    print(f\"{word}: {score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561748bd",
   "metadata": {},
   "source": [
    "\n",
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62fa274e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words with the highest scores for the train set:\n",
      "relatorio: 0.0662\n",
      "clinico: 0.0640\n",
      "nao: 0.0308\n",
      "epilepsia: 0.0282\n",
      "ha: 0.0257\n",
      "alteracoes: 0.0255\n",
      "mg: 0.0237\n",
      "neurologia: 0.0223\n",
      "demencia: 0.0194\n",
      "se: 0.0191\n",
      "\n",
      "Top 10 words with the highest scores for the test set:\n",
      "relatorio: 0.0588\n",
      "clinico: 0.0554\n",
      "epilepsia: 0.0350\n",
      "alteracoes: 0.0350\n",
      "nao: 0.0269\n",
      "tremor: 0.0243\n",
      "ha: 0.0240\n",
      "informacao: 0.0228\n",
      "memoria: 0.0227\n",
      "neurologia: 0.0215\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_set_filtered['filtered_text'], train_set_filtered['result'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the TF-IDF vectorizer object\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, min_df=5)\n",
    "\n",
    "# Fit and transform the vectorizer on the train set\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Extract the top 10 words with the highest scores for the train set\n",
    "train_scores = np.asarray(X_train_tfidf.mean(axis=0)).ravel().tolist()\n",
    "train_features = tfidf_vectorizer.get_feature_names()\n",
    "top_train_indices = np.argsort(train_scores)[::-1][:10]\n",
    "top_train_words = [train_features[i] for i in top_train_indices]\n",
    "top_train_scores = [train_scores[i] for i in top_train_indices]\n",
    "\n",
    "# Transform the vectorizer on the test set\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Extract the top 10 words with the highest scores for the test set\n",
    "test_scores = np.asarray(X_test_tfidf.mean(axis=0)).ravel().tolist()\n",
    "test_features = tfidf_vectorizer.get_feature_names()\n",
    "top_test_indices = np.argsort(test_scores)[::-1][:10]\n",
    "top_test_words = [test_features[i] for i in top_test_indices]\n",
    "top_test_scores = [test_scores[i] for i in top_test_indices]\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 10 words with the highest scores for the train set:\")\n",
    "for word, score in zip(top_train_words, top_train_scores):\n",
    "    print(f\"{word}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 10 words with the highest scores for the test set:\")\n",
    "for word, score in zip(top_test_words, top_test_scores):\n",
    "    print(f\"{word}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227fc248",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
