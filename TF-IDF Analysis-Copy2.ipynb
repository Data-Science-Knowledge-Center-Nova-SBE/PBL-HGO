{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "883aa1a1",
   "metadata": {},
   "source": [
    "# TF-IDF Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95d1c94",
   "metadata": {},
   "source": [
    "TF-IDF (term frequency-inverse document frequency) analysis is a statistical technique used in natural language processing and information retrieval to determine the importance of a word in a document or corpus. It is a way to measure how relevant a word is to a document in a collection of documents.\n",
    "\n",
    "TF-IDF analysis assigns a weight to each word in a document based on how frequently it appears in the document (term frequency) and how rare it is in the entire corpus (inverse document frequency). The weight assigned to a word increases proportionally with its frequency in the document, but is offset by the rarity of the word in the corpus. This means that words that appear frequently in a document but also appear frequently in many other documents in the corpus are given a lower weight, while words that appear less frequently in the corpus but frequently in a particular document are given a higher weight.\n",
    "\n",
    "The output of TF-IDF analysis is a numerical representation of each document that captures the importance of each word in that document. This can be used for various tasks such as text classification, clustering, and information retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ffe588",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Connect to Database ](#Connect-to-database)\n",
    "* [Import Datasets](#Import-Dataset)\n",
    "* [Remove Stopwords](#Remove-stopwords)\n",
    "* [Lemmatization](#Lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b973cc15",
   "metadata": {},
   "source": [
    "## Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee28e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "#creds = [\"username\",\"password\",\"juliehaegh\",\"ninG20&19rea\",\"3306\"] \n",
    "creds = [\"juliehaegh\",\"ninG20&19rea\",\"172.20.20.4\",\"hgo\",3306]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11097233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables: [('ConsultaUrgencia_doentespedidosconsultaNeurologia2012',), ('consultaneurologia2012',), ('consultaneurologia201216anon_true',), ('hgo_data_032023',)]\n",
      "364\n"
     ]
    }
   ],
   "source": [
    "#Connection to the database\n",
    "host = creds[2]\n",
    "user = creds[0]\n",
    "password = creds[1]\n",
    "database = creds[3]\n",
    "port = creds[4]\n",
    "mydb = mysql.connector.connect(host=host, user=user, database=database, port=port, password=password, auth_plugin='mysql_native_password')\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "#Safecheck to guarantee that the connection worked\n",
    "mycursor.execute('SHOW TABLES;')\n",
    "print(f\"Tables: {mycursor.fetchall()}\")\n",
    "print(mydb.connection_id) #it'll give connection_id,if got connected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0b9ef7",
   "metadata": {},
   "source": [
    "## Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a031a49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliehaegh/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/Users/juliehaegh/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import Alert P1 dataset\n",
    "SClinic = pd.read_sql(\"\"\"SELECT * FROM ConsultaUrgencia_doentespedidosconsultaNeurologia2012\"\"\",mydb)\n",
    "\n",
    "# Import SClinic\n",
    "AlertP1 = pd.read_sql(\"\"\"SELECT * FROM consultaneurologia201216anon_true\"\"\",mydb)\n",
    "\n",
    "# Replace all NaN with 0\n",
    "AlertP1 = AlertP1.fillna(0)\n",
    "\n",
    "# Add result column\n",
    "AlertP1['result'] = ['Accepted' if x in [0,14,25,20,53,8,12,12] else 'Refused' for x in AlertP1['COD_MOTIVO_RECUSA']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0215a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with accepted and rejected cases\n",
    "#SClinic['Accepted/Rejected'] = SClinic['COD_MOTIVO_RECUSA'].apply(lambda x: 'Accepted' if x == 0 else 'Rejected')\n",
    "#SClinic = SClinic[(SClinic['Texto']!='') & (SClinic['Accepted/Rejected']=='Accepted')].iloc[887:987]\n",
    "#SClinic = SClinic[SClinic['Texto']!='']\n",
    "#SClinic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca45849f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1540    01/02/2012\n",
       "525     01/02/2013\n",
       "121     01/02/2016\n",
       "168     01/02/2016\n",
       "1154    01/03/2012\n",
       "           ...    \n",
       "437     31/10/2013\n",
       "1051    31/10/2013\n",
       "830     31/10/2014\n",
       "690     31/10/2016\n",
       "1698    31/12/2013\n",
       "Name: DATA_RECEPCAO, Length: 1773, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Split data into train and test\n",
    "AlertP1_sorted = AlertP1[AlertP1['Texto']!=''].sort_values(by='DATA_RECEPCAO')\n",
    "AlertP1_sorted.DATA_RECEPCAO\n",
    "# calculate the index for the split\n",
    "#split_index = math.ceil(0.8 * len(AlertP1_sorted))\n",
    "\n",
    "# split the data frame into test and train sets\n",
    "#train_set = AlertP1_sorted.iloc[:split_index]\n",
    "#test_set = AlertP1_sorted.iloc[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d4e96a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check the balance of result feature:\n",
    "AlertP1_sorted['result'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2b7ea14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juliehaegh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/juliehaegh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import librariers \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a088fb",
   "metadata": {},
   "source": [
    "## Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9b4873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of special characters and transform Texto column to Latin words\n",
    "#train_set['Texto'] = train_set['Texto'].apply(lambda x: unidecode(x))\n",
    "#test_set['Texto'] = test_set['Texto'].apply(lambda x: unidecode(x))\n",
    "AlertP1_sorted['Texto'] = AlertP1_sorted['Texto'].apply(lambda x: unidecode(x))\n",
    "\n",
    "#The re.sub function is used to substitute all digits (\\d) with an empty string\n",
    "#train_set['Texto'] = train_set['Texto'].apply(lambda x: re.sub(r'\\d', '', x))\n",
    "#test_set['Texto'] = test_set['Texto'].apply(lambda x: re.sub(r'\\d', '', x))\n",
    "AlertP1_sorted['Texto'] = AlertP1_sorted['Texto'].apply(lambda x: re.sub(r'\\d', '', x))\n",
    "\n",
    "# Remove all names in Texto variable\n",
    "# This function uses a regular expression to find all words in the text that start with a \n",
    "# capital letter (\\b[A-Z][a-z]+\\b), which are assumed to be names\n",
    "#text = train_set['Texto'] \n",
    "text = AlertP1_sorted['Texto'] \n",
    "\n",
    "# remove all hyphens from the text\n",
    "text = text.replace('-', '')\n",
    "\n",
    "def remove_names(text):\n",
    "    # Find all words that start with a capital letter\n",
    "    names = re.findall(r'\\b[A-Z][a-z]+\\b', text)\n",
    "    \n",
    "    # Replace the names with an empty string\n",
    "    for name in names:\n",
    "        text = text.replace(name, '')\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2069e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the text\n",
    "text_list = []\n",
    "\n",
    "# Loop through the 'text' column\n",
    "for text in text.str.lower(): # Transform every word to lower case\n",
    "    text_list.append(text)\n",
    "\n",
    "# Print the list of text\n",
    "#print(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2e7d46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juliehaegh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/juliehaegh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the Portuguese stop words\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Get the Portuguese stop words\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Manually remove stopwords\n",
    "stop_words.update(['-//','.', ',','(',')',':','-','?','+','/',';','2','1','drª','``','','3','desde','anos','doente','consulta','alterações','se',\"''\",'cerca','refere','hgo','utente','vossa','s','...','ainda','c','filha','costa','dr.','pereira','ja','--','p','dr','h','n','>','q','//','..','b','++','%','//','-','+++/','=','+++/'])\n",
    "\n",
    "# Create a new list to store the filtered text\n",
    "filtered_text = []\n",
    "\n",
    "# Loop through the text_list and remove the stop words\n",
    "for text in text_list:\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    filtered_text.append(\" \".join(words))\n",
    "\n",
    "# Print the filtered text\n",
    "#print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b63efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the filtered text as a new column to the dataframe\n",
    "AlertP1_sorted['filtered_text'] = filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95691b55",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e87d5",
   "metadata": {},
   "source": [
    "Lemmatization is a text normalization technique used in Natural Language Processing (NLP), that switches any kind of a word to its base root mode. Lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "449168bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for lemmatization\n",
    "def spacy_lemmatizer(df):\n",
    "    import spacy\n",
    "    import pt_core_news_md\n",
    "    nlp = pt_core_news_md.load()\n",
    "\n",
    "    doclist = list(nlp.pipe(df))\n",
    "\n",
    "    docs=[]\n",
    "    for i, doc in enumerate(doclist):\n",
    "        docs.append(' '.join([listitem.lemma_ for listitem in doc]))\n",
    "        \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c39bfb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list to store the words\n",
    "word_list = []\n",
    "\n",
    "# loop through each row of the \"text_column\" column\n",
    "for index, row in AlertP1_sorted.iterrows():\n",
    "    \n",
    "    # split the text into individual words using whitespace as a delimiter\n",
    "    words = row['filtered_text'].split()\n",
    "    # add the words to the word list\n",
    "    word_list.extend(words)\n",
    "\n",
    "# print the word list\n",
    "#print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2657afaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list to store the words\n",
    "word_list = []\n",
    "\n",
    "# loop through each row of the \"text_column\" column\n",
    "for index, row in AlertP1_sorted.iterrows():\n",
    "    \n",
    "    # split the text into individual words using whitespace as a delimiter\n",
    "    words = row['filtered_text'].split()\n",
    "    \n",
    "    # remove hyphens from the words and add them to the word list\n",
    "    word_list.extend([word.replace('-', '') for word in words])\n",
    "    # remove slash from the words and ass them to the list\n",
    "    word_list.extend([word.replace('/', '') for word in words])\n",
    "    \n",
    "\n",
    "# print the cleaned word list\n",
    "#print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56096fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words after lemmatization: 9884\n",
      "The number of words before lemmatization: 12057\n"
     ]
    }
   ],
   "source": [
    "Lemma = spacy_lemmatizer(word_list) # Call lemmatizer function\n",
    "\n",
    "# print length of word_list and compare the count after doing lemmatization\n",
    "from collections import Counter\n",
    "\n",
    "items = Counter(Lemma).keys()\n",
    "print('The number of words after lemmatization:',len(items))\n",
    "\n",
    "items2 = Counter(word_list).keys()\n",
    "print('The number of words before lemmatization:',len(items2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98955bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>filtered_text</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>dor lapso efoi-le dar alto qualquer justificac...</td>\n",
       "      <td>dor lapso foi-lhe dada alta qualquer justifica...</td>\n",
       "      <td>Refused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>relatorio clinico</td>\n",
       "      <td>relatorio clinico</td>\n",
       "      <td>Refused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>homem ap dm gamapatia monoclonal igm dca arter...</td>\n",
       "      <td>homem ap dm gamapatia monoclonal igm dca arter...</td>\n",
       "      <td>Refused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>mulher dor ponto lingua sensacao repuxamento l...</td>\n",
       "      <td>mulher dor ponta lingua sensacao repuxamento l...</td>\n",
       "      <td>Refused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>epilepsia</td>\n",
       "      <td>epilepsia</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>queixa tontura episodio perturbacoes equilibri...</td>\n",
       "      <td>queixas tonturas episodios perturbacoes equili...</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>esclerose multiplo</td>\n",
       "      <td>esclerose multipla</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>cefaleia parietal intenso nunca desequilibriar...</td>\n",
       "      <td>cefaleia parietal intensa nunca desequilibrio ...</td>\n",
       "      <td>Refused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>cidp sob corticoterapia</td>\n",
       "      <td>cidp sob corticoterapia</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>relatorio clinico</td>\n",
       "      <td>relatorio clinico</td>\n",
       "      <td>Refused</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1768 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text_lemmatized  \\\n",
       "1540  dor lapso efoi-le dar alto qualquer justificac...   \n",
       "525                                   relatorio clinico   \n",
       "121   homem ap dm gamapatia monoclonal igm dca arter...   \n",
       "168   mulher dor ponto lingua sensacao repuxamento l...   \n",
       "1154                                          epilepsia   \n",
       "...                                                 ...   \n",
       "437   queixa tontura episodio perturbacoes equilibri...   \n",
       "1051                                 esclerose multiplo   \n",
       "830   cefaleia parietal intenso nunca desequilibriar...   \n",
       "690                             cidp sob corticoterapia   \n",
       "1698                                  relatorio clinico   \n",
       "\n",
       "                                          filtered_text    result  \n",
       "1540  dor lapso foi-lhe dada alta qualquer justifica...   Refused  \n",
       "525                                   relatorio clinico   Refused  \n",
       "121   homem ap dm gamapatia monoclonal igm dca arter...   Refused  \n",
       "168   mulher dor ponta lingua sensacao repuxamento l...   Refused  \n",
       "1154                                          epilepsia  Accepted  \n",
       "...                                                 ...       ...  \n",
       "437   queixas tonturas episodios perturbacoes equili...  Accepted  \n",
       "1051                                 esclerose multipla  Accepted  \n",
       "830   cefaleia parietal intensa nunca desequilibrio ...   Refused  \n",
       "690                             cidp sob corticoterapia  Accepted  \n",
       "1698                                  relatorio clinico   Refused  \n",
       "\n",
       "[1768 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the spacy_lemmatizer function to each row in the 'text' column\n",
    "AlertP1_sorted['text_lemmatized'] = spacy_lemmatizer(AlertP1_sorted['filtered_text'])\n",
    "\n",
    "# drop rows with empty strings\n",
    "AlertP1_filtered = AlertP1_sorted[['text_lemmatized','filtered_text','result']].replace('', pd.NA).dropna()\n",
    "AlertP1_filtered = pd.DataFrame(AlertP1_filtered)\n",
    "AlertP1_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d3b2f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>filtered_text</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>dor lapso efoi-le dar alto qualquer justificac...</td>\n",
       "      <td>dor lapso foi-lhe dada alta qualquer justifica...</td>\n",
       "      <td>Refused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>relatorio clinico</td>\n",
       "      <td>relatorio clinico</td>\n",
       "      <td>Refused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>homem ap dm gamapatia monoclonal igm dca arter...</td>\n",
       "      <td>homem ap dm gamapatia monoclonal igm dca arter...</td>\n",
       "      <td>Refused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>mulher dor ponto lingua sensacao repuxamento l...</td>\n",
       "      <td>mulher dor ponta lingua sensacao repuxamento l...</td>\n",
       "      <td>Refused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>sexo feminino relatar sentir se cada vez esque...</td>\n",
       "      <td>sexo feminino relata sentir-se cada vez esquec...</td>\n",
       "      <td>Refused</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text_lemmatized  \\\n",
       "1540  dor lapso efoi-le dar alto qualquer justificac...   \n",
       "525                                   relatorio clinico   \n",
       "121   homem ap dm gamapatia monoclonal igm dca arter...   \n",
       "168   mulher dor ponto lingua sensacao repuxamento l...   \n",
       "502   sexo feminino relatar sentir se cada vez esque...   \n",
       "\n",
       "                                          filtered_text   result  \n",
       "1540  dor lapso foi-lhe dada alta qualquer justifica...  Refused  \n",
       "525                                   relatorio clinico  Refused  \n",
       "121   homem ap dm gamapatia monoclonal igm dca arter...  Refused  \n",
       "168   mulher dor ponta lingua sensacao repuxamento l...  Refused  \n",
       "502   sexo feminino relata sentir-se cada vez esquec...  Refused  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refused_referrals = AlertP1_filtered[AlertP1_filtered.result == 'Refused']\n",
    "refused_referrals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3101eedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>filtered_text</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>epilepsia</td>\n",
       "      <td>epilepsia</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>sintomatologia mulher hipertenso epilepsia .qu...</td>\n",
       "      <td>sintomatologia mulher hipertensa epilepsia .qu...</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>sexo masculino ap epilepsia medicar tegretol s...</td>\n",
       "      <td>sexo masculino ap epilepsia medicado tegretol ...</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>homem quadro alucinacoes visual bem delinear a...</td>\n",
       "      <td>homem quadro alucinacoes visuais bem delineada...</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>perda progressivo equilibrio marchar dificulda...</td>\n",
       "      <td>perda progressiva equilibrio marcha dificuldad...</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text_lemmatized  \\\n",
       "1154                                          epilepsia   \n",
       "1151  sintomatologia mulher hipertenso epilepsia .qu...   \n",
       "618   sexo masculino ap epilepsia medicar tegretol s...   \n",
       "782   homem quadro alucinacoes visual bem delinear a...   \n",
       "795   perda progressivo equilibrio marchar dificulda...   \n",
       "\n",
       "                                          filtered_text    result  \n",
       "1154                                          epilepsia  Accepted  \n",
       "1151  sintomatologia mulher hipertensa epilepsia .qu...  Accepted  \n",
       "618   sexo masculino ap epilepsia medicado tegretol ...  Accepted  \n",
       "782   homem quadro alucinacoes visuais bem delineada...  Accepted  \n",
       "795   perda progressiva equilibrio marcha dificuldad...  Accepted  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accepted_referrals = AlertP1_filtered[AlertP1_filtered.result == 'Accepted']\n",
    "accepted_referrals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff3559b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of x_train:  (1414, 10323)\n",
      "Size of x_test:  (354, 10323)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = AlertP1_filtered.filtered_text\n",
    "Y = AlertP1_filtered.result\n",
    "\n",
    "vect = TfidfVectorizer()\n",
    "X = vect.fit_transform(AlertP1_filtered.filtered_text)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size= 0.2, random_state=42)\n",
    "\n",
    "print('Size of x_train: ', (x_train.shape))\n",
    "#print('Size of y_train: ,' (y_train.shape))\n",
    "print('Size of x_test: ', (x_test.shape))\n",
    "#print('Size of y_test: ,' (y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98a08c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 72.88%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train, y_train)\n",
    "logreg_pred = logreg.predict(x_test)\n",
    "logreg_acc = accuracy_score(logreg_pred, y_test)\n",
    "print('Test accuracy: {:.2f}%'. format(logreg_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9500d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[218   4]\n",
      " [ 92  40]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Accepted       0.70      0.98      0.82       222\n",
      "     Refused       0.91      0.30      0.45       132\n",
      "\n",
      "    accuracy                           0.73       354\n",
      "   macro avg       0.81      0.64      0.64       354\n",
      "weighted avg       0.78      0.73      0.68       354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, logreg_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, logreg_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e7e2895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 71.19%\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(x_train, y_train)\n",
    "mnb_pred = mnb.predict(x_test)\n",
    "mnb_acc = accuracy_score(mnb_pred, y_test)\n",
    "print('Test accuracy: {:.2f}%'. format(mnb_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30ee626d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[218   4]\n",
      " [ 98  34]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Accepted       0.69      0.98      0.81       222\n",
      "     Refused       0.89      0.26      0.40       132\n",
      "\n",
      "    accuracy                           0.71       354\n",
      "   macro avg       0.79      0.62      0.61       354\n",
      "weighted avg       0.77      0.71      0.66       354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, mnb_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, mnb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07727da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 69.77%\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC()\n",
    "svc.fit(x_train, y_train)\n",
    "svc_pred = svc.predict(x_test)\n",
    "svc_acc = accuracy_score(svc_pred, y_test)\n",
    "print('Test accuracy: {:.2f}%'. format(svc_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8602b086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[189  33]\n",
      " [ 74  58]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Accepted       0.72      0.85      0.78       222\n",
      "     Refused       0.64      0.44      0.52       132\n",
      "\n",
      "    accuracy                           0.70       354\n",
      "   macro avg       0.68      0.65      0.65       354\n",
      "weighted avg       0.69      0.70      0.68       354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, svc_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, svc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4298d396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV 1/5] END .................C=0.1, loss=hinge;, score=0.714 total time=   0.0s\n",
      "[CV 2/5] END .................C=0.1, loss=hinge;, score=0.703 total time=   0.0s\n",
      "[CV 3/5] END .................C=0.1, loss=hinge;, score=0.710 total time=   0.0s\n",
      "[CV 4/5] END .................C=0.1, loss=hinge;, score=0.693 total time=   0.0s\n",
      "[CV 5/5] END .................C=0.1, loss=hinge;, score=0.713 total time=   0.0s\n",
      "[CV 1/5] END .........C=0.1, loss=squared_hinge;, score=0.721 total time=   0.0s\n",
      "[CV 2/5] END .........C=0.1, loss=squared_hinge;, score=0.717 total time=   0.0s\n",
      "[CV 3/5] END .........C=0.1, loss=squared_hinge;, score=0.710 total time=   0.0s\n",
      "[CV 4/5] END .........C=0.1, loss=squared_hinge;, score=0.693 total time=   0.0s\n",
      "[CV 5/5] END .........C=0.1, loss=squared_hinge;, score=0.709 total time=   0.0s\n",
      "[CV 1/5] END ...................C=1, loss=hinge;, score=0.735 total time=   0.0s\n",
      "[CV 2/5] END ...................C=1, loss=hinge;, score=0.721 total time=   0.0s\n",
      "[CV 3/5] END ...................C=1, loss=hinge;, score=0.735 total time=   0.0s\n",
      "[CV 4/5] END ...................C=1, loss=hinge;, score=0.686 total time=   0.0s\n",
      "[CV 5/5] END ...................C=1, loss=hinge;, score=0.702 total time=   0.0s\n",
      "[CV 1/5] END ...........C=1, loss=squared_hinge;, score=0.703 total time=   0.0s\n",
      "[CV 2/5] END ...........C=1, loss=squared_hinge;, score=0.696 total time=   0.0s\n",
      "[CV 3/5] END ...........C=1, loss=squared_hinge;, score=0.703 total time=   0.0s\n",
      "[CV 4/5] END ...........C=1, loss=squared_hinge;, score=0.686 total time=   0.0s\n",
      "[CV 5/5] END ...........C=1, loss=squared_hinge;, score=0.681 total time=   0.0s\n",
      "[CV 1/5] END ..................C=10, loss=hinge;, score=0.682 total time=   0.0s\n",
      "[CV 2/5] END ..................C=10, loss=hinge;, score=0.682 total time=   0.0s\n",
      "[CV 3/5] END ..................C=10, loss=hinge;, score=0.675 total time=   0.0s\n",
      "[CV 4/5] END ..................C=10, loss=hinge;, score=0.618 total time=   0.0s\n",
      "[CV 5/5] END ..................C=10, loss=hinge;, score=0.663 total time=   0.0s\n",
      "[CV 1/5] END ..........C=10, loss=squared_hinge;, score=0.678 total time=   0.0s\n",
      "[CV 2/5] END ..........C=10, loss=squared_hinge;, score=0.689 total time=   0.0s\n",
      "[CV 3/5] END ..........C=10, loss=squared_hinge;, score=0.696 total time=   0.0s\n",
      "[CV 4/5] END ..........C=10, loss=squared_hinge;, score=0.625 total time=   0.1s\n",
      "[CV 5/5] END ..........C=10, loss=squared_hinge;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END .................C=100, loss=hinge;, score=0.671 total time=   0.1s\n",
      "[CV 2/5] END .................C=100, loss=hinge;, score=0.657 total time=   0.1s\n",
      "[CV 3/5] END .................C=100, loss=hinge;, score=0.664 total time=   0.1s\n",
      "[CV 4/5] END .................C=100, loss=hinge;, score=0.618 total time=   0.1s\n",
      "[CV 5/5] END .................C=100, loss=hinge;, score=0.656 total time=   0.1s\n",
      "[CV 1/5] END .........C=100, loss=squared_hinge;, score=0.675 total time=   0.1s\n",
      "[CV 2/5] END .........C=100, loss=squared_hinge;, score=0.654 total time=   0.1s\n",
      "[CV 3/5] END .........C=100, loss=squared_hinge;, score=0.661 total time=   0.1s\n",
      "[CV 4/5] END .........C=100, loss=squared_hinge;, score=0.622 total time=   0.1s\n",
      "[CV 5/5] END .........C=100, loss=squared_hinge;, score=0.649 total time=   0.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LinearSVC(),\n",
       "             param_grid={'C': [0.1, 1, 10, 100],\n",
       "                         'loss': ['hinge', 'squared_hinge']},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C':[0.1, 1, 10, 100], 'loss':['hinge','squared_hinge']}\n",
    "grid = GridSearchCV(svc, param_grid, refit=True, verbose=3)\n",
    "grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d15fdf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross validation score: 0.72\n",
      "best parameters {'C': 1, 'loss': 'hinge'}\n"
     ]
    }
   ],
   "source": [
    "print('best cross validation score: {:.2f}'. format(grid.best_score_))\n",
    "print('best parameters', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a00698d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 73.16%\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC(C = 1, loss='hinge')\n",
    "svc.fit(x_train, y_train)\n",
    "svc_pred = svc.predict(x_test)\n",
    "svc_acc = accuracy_score(svc_pred, y_test)\n",
    "print('Test accuracy: {:.2f}%'. format(svc_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e8d5c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[207  15]\n",
      " [ 80  52]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Accepted       0.72      0.93      0.81       222\n",
      "     Refused       0.78      0.39      0.52       132\n",
      "\n",
      "    accuracy                           0.73       354\n",
      "   macro avg       0.75      0.66      0.67       354\n",
      "weighted avg       0.74      0.73      0.70       354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, svc_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, svc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227fc248",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e18fd1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03054053",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
